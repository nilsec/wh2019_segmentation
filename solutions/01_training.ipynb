{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 0.1\n",
    "## Introduction to EM data, neuron segmentation and gunpowder\n",
    "### A simple gunpowder example pipeline for loading and manipulating data\n",
    "\n",
    "Become familiar with gunpowder. Read and try to understand the following code as well as the general principle behind gunpowder. Read the introductory example in the gunpowder documentation http://funkey.science/gunpowder and gunpowder tutorial http://funkey.science/gunpowder/tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from gunpowder import *\n",
    "from gunpowder.tensorflow import *\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# A simple gunpowder pipeline for loading and manipulating data:\n",
    "data_dir = '../../jan/segmentation/data'\n",
    "\n",
    "# Define gunpowder variables of interest:\n",
    "raw = ArrayKey('RAW') # Raw EM data\n",
    "labels = ArrayKey('GT_LABELS') # ground truth neuron segmentation \n",
    "affinities = ArrayKey('GT_AFFINITIES') # affinities\n",
    "\n",
    "# Voxel size is the physical size of one voxel (=3D pixel) in nm.\n",
    "voxel_size = Coordinate((40, 4, 4))\n",
    "batch_size = Coordinate((50,1000,1000)) * voxel_size\n",
    "\n",
    "# Request all the data you need for training:\n",
    "request = BatchRequest()\n",
    "request.add(raw, batch_size)\n",
    "request.add(labels, batch_size)\n",
    "request.add(affinities, batch_size)\n",
    "\n",
    "selec_roi = Roi(offset=(3000,5000,5100), shape=batch_size)\n",
    "request[raw].roi = selec_roi\n",
    "request[labels].roi = selec_roi\n",
    "request[affinities].roi = selec_roi\n",
    "\n",
    "# Request a snapshot s.t. you are able to visualize the data in your pipeline.\n",
    "snapshot_request = BatchRequest({\n",
    "    raw: request[raw],\n",
    "    labels: request[labels],\n",
    "    affinities: request[affinities]\n",
    "})\n",
    "\n",
    "# Note that the data only provides raw and neuron_ids which is the neuron segmentation.\n",
    "# However, we need affinities which we can generate from neuron_ids by using gunpowder (see below):\n",
    "data_sources = tuple(\n",
    "    N5Source(\n",
    "        os.path.join(data_dir, sample + '.n5'),\n",
    "        datasets = {\n",
    "            raw: 'volumes/raw',\n",
    "            labels: 'volumes/labels/neuron_ids',\n",
    "        },\n",
    "        array_specs = {\n",
    "            raw: ArraySpec(interpolatable=True),\n",
    "            labels: ArraySpec(interpolatable=False)\n",
    "        }\n",
    "    ) +\n",
    "    Normalize(raw)\n",
    "    for sample in [\"sample_C\"]\n",
    "    )\n",
    "\n",
    "# Define a neighborhood for affinities:\n",
    "neighborhood = [[-1, 0, 0], [0, -1, 0], [0, 0, -1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# Build the actual data-pipeline:\n",
    "\n",
    "def basic_pipeline():\n",
    "    pipeline = (\n",
    "        data_sources +\n",
    "        RandomProvider() +\n",
    "        AddAffinities(neighborhood,\n",
    "                     labels=labels,\n",
    "                     affinities=affinities) +\n",
    "        Snapshot(dataset_names={raw: 'volumes/raw',\n",
    "                  labels: 'volumes/labels/neuron_ids',\n",
    "                  affinities: 'volumes/affinities'},\n",
    "                  output_filename=\"snapshot_basic.hdf\"))\n",
    "    return pipeline\n",
    "\n",
    "def simple_augment_pipeline():\n",
    "    pipeline = (\n",
    "        data_sources +\n",
    "        RandomProvider() +\n",
    "        SimpleAugment(transpose_only=[1, 2]) +\n",
    "        AddAffinities(neighborhood,\n",
    "                     labels=labels,\n",
    "                     affinities=affinities) +\n",
    "        Snapshot(dataset_names={raw: 'volumes/raw',\n",
    "                  labels: 'volumes/labels/neuron_ids',\n",
    "                  affinities: 'volumes/affinities'},\n",
    "                  output_filename=\"snapshot_simple.hdf\"))\n",
    "    return pipeline\n",
    "\n",
    "def intensity_augment_pipeline():\n",
    "    pipeline = (\n",
    "        data_sources +\n",
    "        RandomProvider() +\n",
    "        IntensityAugment(raw, 0.9, 1.1, -0.1, 0.1, z_section_wise=True) +    \n",
    "        AddAffinities(neighborhood,\n",
    "                     labels=labels,\n",
    "                     affinities=affinities) +\n",
    "        Snapshot(dataset_names={raw: 'volumes/raw',\n",
    "                  labels: 'volumes/labels/neuron_ids',\n",
    "                  affinities: 'volumes/affinities'},\n",
    "                  output_filename=\"snapshot_intensity.hdf\"))\n",
    "    return pipeline\n",
    "\n",
    "def elastic_augment_pipeline():\n",
    "    pipeline = (\n",
    "        data_sources +\n",
    "        RandomProvider() +\n",
    "        ElasticAugment(\n",
    "            control_point_spacing=[4,40,40],\n",
    "            jitter_sigma=[0,2,2],\n",
    "            rotation_interval=[0,math.pi/2.0],\n",
    "            prob_slip=0.05,\n",
    "            prob_shift=0.05,\n",
    "            max_misalign=10,\n",
    "            subsample=8) +\n",
    "        AddAffinities(neighborhood,\n",
    "                     labels=labels,\n",
    "                     affinities=affinities) +\n",
    "        Snapshot(dataset_names={raw: 'volumes/raw',\n",
    "                  labels: 'volumes/labels/neuron_ids',\n",
    "                  affinities: 'volumes/affinities'},\n",
    "                  output_filename=\"snapshot_elastic.hdf\"))\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def king_of_the_north():\n",
    "    pipeline = (\n",
    "        data_sources +\n",
    "        RandomProvider() +\n",
    "        ElasticAugment(\n",
    "            control_point_spacing=[4,40,40],\n",
    "            jitter_sigma=[0,2,2],\n",
    "            rotation_interval=[0,math.pi/2.0],\n",
    "            prob_slip=0.05,\n",
    "            prob_shift=0.05,\n",
    "            max_misalign=10,\n",
    "            subsample=8) +\n",
    "        SimpleAugment(transpose_only=[1, 2]) +\n",
    "        IntensityAugment(raw, 0.9, 1.1, -0.1, 0.1, z_section_wise=True) +\n",
    "        AddAffinities(neighborhood,\n",
    "                     labels=labels,\n",
    "                     affinities=affinities) +\n",
    "        Snapshot(dataset_names={raw: 'volumes/raw',\n",
    "                  labels: 'volumes/labels/neuron_ids',\n",
    "                  affinities: 'volumes/affinities'},\n",
    "                  output_filename=\"snapshot_king_of_the_north.hdf\"))\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "pipeline_setups={\"basic\": basic_pipeline,\n",
    "                 \"simple\": simple_augment_pipeline,\n",
    "                 \"intensity\": intensity_augment_pipeline,\n",
    "                 \"elastic\": elastic_augment_pipeline}\n",
    "\n",
    "try:\n",
    "    pipeline_setups.update({\"king_of_the_north\": king_of_the_north})\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 0.2\n",
    "## Choose a pipeline and understand how different augmentation types influence the output.\n",
    "\n",
    "### You can choose between:\n",
    "1. basic (No augmentations)\n",
    "2. simple\n",
    "3. intensity \n",
    "4. elastic\n",
    "5. (*If you implemented it: king_of_the_north)\n",
    "\n",
    "Try to find out what each of these are actually doing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a pipeline, run it and observe the differences in neuroglancer.\n",
    "pipeline_name = \"king_of_the_north\"\n",
    "\n",
    "if not os.path.exists(\"./snapshots\"):\n",
    "    os.makedirs(\"./snapshots\")\n",
    "pipeline_setup = pipeline_setups[pipeline_name]\n",
    "\n",
    "with build(pipeline_setup()) as b:\n",
    "    b.request_batch(request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize data using neuroglancer\n",
    "\n",
    "The prior cell generated a file in your working directory containg the raw EM data, neuron segmentation and affinities. The following script lets you view the data you just generated and will change depending on which augmentation pipeline you choose. Run both cells below and click on the resulting link. You can enable and disable layers by clicking on them. Explore the data. What do the different colors of the affinities mean? How does it relate to the segmentation? How is augmentation affecting the data?\n",
    "\n",
    "If you double click on a particular segment neuroglancer allows you to view the 3D mesh of the object.\n",
    "\n",
    "### Dataset:\n",
    "The data you are looking at are a tiny subset of the **whole** adult Drosophila brain, imaged via serial section transmission electron microscopy (SSTEM) ([Zheng et al. 2018](https://www.sciencedirect.com/science/article/pii/S0092867418307876)). If you are curious and want to get a sense of the scale, [here](https://fafb.catmaid.virtualflybrain.org/?pid=1&zp=127040&yp=253133.36932477387&xp=585045.0300033365&tool=navigator&sid0=1&s0=7.400000000000001) you can browse the entire dataset. Zooming in and out is a lot of fun, don't get lost.\n",
    "\n",
    "The particular cutout you are looking at here, is itself a small cutout of the [CREMI](https://cremi.org/) challenge volumes: a neuron and synapse segmentation challenge that provides manually acquired ground truth and provides a way to quantitatively compare competing algorithmic approaches.\n",
    "\n",
    "##### REPLACE THE IP BELOW WITH YOURS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuroglancer\n",
    "import funlib.show.neuroglancer as funlibng\n",
    "import daisy\n",
    "import numpy as np\n",
    "\n",
    "# !!! IMPORTANT: Replace my_ip with your public paperspace ip as shown in your browser !!!\n",
    "\n",
    "def view_snapshot(pipeline_name, my_ip=\"74.82.31.73\", snapshot_file=\"./snapshots/snapshot\"):\n",
    "    neuroglancer.set_server_bind_address('0.0.0.0', 8889)\n",
    "    snapshot_file = snapshot_file + \"_\" + pipeline_name + \".hdf\"\n",
    "    \n",
    "    affs_ds = daisy.open_ds(snapshot_file, 'volumes/affinities')\n",
    "    affs_ds.data = np.array(affs_ds.data, dtype=np.float32)\n",
    "    labels_ds = daisy.open_ds(snapshot_file, 'volumes/labels/neuron_ids')\n",
    "    raw_ds = daisy.open_ds(snapshot_file, 'volumes/raw')\n",
    "\n",
    "    neuroglancer.set_server_bind_address('0.0.0.0')\n",
    "    viewer = neuroglancer.Viewer()\n",
    "    with viewer.txn() as s:\n",
    "        funlibng.add_layer(s, raw_ds, 'raw')\n",
    "        funlibng.add_layer(s, labels_ds, 'labels')\n",
    "        funlibng.add_layer(s, affs_ds, 'affinities', shader='rgb')\n",
    "\n",
    "    print(viewer.__str__().replace(\"localhost\", my_ip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://74.82.31.73:36615/v/a31d6e977d47b918bb5a6157e0accfd3ce39800b/\n"
     ]
    }
   ],
   "source": [
    "view_snapshot(pipeline_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "## Using gunpowder with tensorflow for training a 3D-UNet for affinity prediction\n",
    "#### mknet.py\n",
    "\n",
    "In this exercise we ask you to pick a network of your choosing to create a computation graph for your neural network. Follow the instructions in the code and make use of help and documentations.\n",
    "\n",
    "As usual if you are not sure about what a function does or what its argument is navigate to the function and press ```shift + tab``` to display arguments and documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6659018488027667204\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the output above doesn't mention the word GPU scream HELP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funlib.learn.tensorflow import models\n",
    "import malis\n",
    "import tensorflow as tf\n",
    "import json\n",
    "\n",
    "\n",
    "def create_network(name, input_shape, num_fmaps, fmap_inc_factors, downsample_factors):\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    with tf.variable_scope('setup0'):\n",
    "\n",
    "        raw = tf.placeholder(tf.float32, shape=input_shape)\n",
    "        raw_batched = tf.reshape(raw, (1, 1) + input_shape)\n",
    "\n",
    "        unet, _, _ = models.unet(\n",
    "                raw_batched,\n",
    "                num_fmaps,\n",
    "                fmap_inc_factors,\n",
    "                downsample_factors)\n",
    "\n",
    "        affs_batched, _ = models.conv_pass(\n",
    "            unet,\n",
    "            kernel_sizes=[1],\n",
    "            num_fmaps=3,\n",
    "            activation='sigmoid',\n",
    "            name='affs')\n",
    "\n",
    "        output_shape_batched = affs_batched.get_shape().as_list()\n",
    "        output_shape = output_shape_batched[1:] # strip the batch dimension\n",
    "\n",
    "        affs = tf.reshape(affs_batched, output_shape)\n",
    "\n",
    "        gt_affs = tf.placeholder(tf.float32, shape=output_shape)\n",
    "        affs_loss_weights = tf.placeholder(tf.float32, shape=output_shape)\n",
    "\n",
    "        loss = tf.losses.mean_squared_error(\n",
    "            gt_affs,\n",
    "            affs,\n",
    "            affs_loss_weights)\n",
    "\n",
    "        summary = tf.summary.scalar('setup0', loss)\n",
    "\n",
    "        opt = tf.train.AdamOptimizer(\n",
    "            learning_rate=0.5e-4,\n",
    "            beta1=0.95,\n",
    "            beta2=0.999,\n",
    "            epsilon=1e-8)\n",
    "        optimizer = opt.minimize(loss)\n",
    "\n",
    "        output_shape = output_shape[1:]\n",
    "        print(\"input shape : %s\"%(input_shape,))\n",
    "        print(\"output shape: %s\"%(output_shape,))\n",
    "\n",
    "        tf.train.export_meta_graph(filename=name + '.meta')\n",
    "\n",
    "        config = {\n",
    "            'raw': raw.name,\n",
    "            'affs': affs.name,\n",
    "            'gt_affs': gt_affs.name,\n",
    "            'affs_loss_weights': affs_loss_weights.name,\n",
    "            'loss': loss.name,\n",
    "            'optimizer': optimizer.name,\n",
    "            'input_shape': input_shape,\n",
    "            'output_shape': output_shape,\n",
    "            'summary': summary.name\n",
    "        }\n",
    "\n",
    "        config['outputs'] = {'affs': {\"out_dims\": 3, \"out_dtype\": \"uint8\"}}\n",
    "\n",
    "        with open(name + '.json', 'w') as f:\n",
    "            json.dump(config, f)\n",
    "        \n",
    "        if name==\"arya\":\n",
    "            print(\"\\nVALAR MORGHULIS\")\n",
    "        if name==\"jon\":\n",
    "            print(\"\\nFOR THE NORTH\")\n",
    "        if name==\"daenerys\":\n",
    "            print(\"\\nDRACARYS\")\n",
    "        if name==\"cersei\":\n",
    "            print(\"\\nJAIME!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you find a selection of your favorite 3D-UNets in Westeros. Make use of the following documentation to decide which of these networks is to your liking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.unet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "arya_stark = {\"name\": \"arya\", \n",
    "        \"input_shape\": (84, 268, 268),\n",
    "        \"num_fmaps\": 24,\n",
    "        \"fmap_inc_factors\": 2,\n",
    "        \"downsample_factors\": [[1,3,3],[4,12,12]]}\n",
    "\n",
    "jon_snow = {\"name\": \"jon\", \n",
    "        \"input_shape\": (84, 268, 268),\n",
    "        \"num_fmaps\": 12,\n",
    "        \"fmap_inc_factors\": 5,\n",
    "        \"downsample_factors\": [[1,3,3],[1,3,3],[3,3,3]]}\n",
    "\n",
    "daenerys_stormborn = {\"name\": \"daenerys\", \n",
    "                      \"input_shape\": (84, 268, 268),\n",
    "                      \"num_fmaps\": 12,\n",
    "                      \"fmap_inc_factors\": 2,\n",
    "                      \"downsample_factors\": [[1,3,3],[1,3,3],[1,1,1],[1,1,1]]}\n",
    "\n",
    "cersei_lennister = {\"name\": \"cersei\", \n",
    "                    \"input_shape\": (84, 268, 268),\n",
    "                    \"num_fmaps\": 100,\n",
    "                    \"fmap_inc_factors\": 1,\n",
    "                    \"downsample_factors\": [[1,3,3],[1,3,3],[3,3,3]]}\n",
    "\n",
    "networks = {\"arya\": arya_stark, \"jon\": jon_snow, \"daenerys\": daenerys_stormborn, \"cersei\": cersei_lennister}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick your network\n",
    "Choose between Arya, Jon, Daenerys and Cersei described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating U-Net layer 0\n",
      "f_in: (1, 1, 84, 268, 268)\n",
      "number of variables added: 272900, new total: 272900\n",
      "    Creating U-Net layer 1\n",
      "    f_in: (1, 100, 80, 88, 88)\n",
      "    number of variables added: 540200, new total: 813100\n",
      "        Creating U-Net layer 2\n",
      "        f_in: (1, 100, 76, 28, 28)\n",
      "        number of variables added: 540200, new total: 1353300\n",
      "            Creating U-Net layer 3\n",
      "            f_in: (1, 100, 24, 8, 8)\n",
      "            bottom layer\n",
      "            f_out: (1, 100, 20, 4, 4)\n",
      "            number of variables added: 540200, new total: 1893500\n",
      "        g_out: (1, 100, 20, 4, 4)\n",
      "        g_out_upsampled: (1, 100, 60, 12, 12)\n",
      "        f_left_cropped: (1, 100, 60, 12, 12)\n",
      "        f_right: (1, 200, 60, 12, 12)\n",
      "        f_out: (1, 100, 56, 8, 8)\n",
      "        number of variables added: 1080300, new total: 2973800\n",
      "    g_out: (1, 100, 56, 8, 8)\n",
      "    g_out_upsampled: (1, 100, 56, 24, 24)\n",
      "    f_left_cropped: (1, 100, 56, 24, 24)\n",
      "    f_right: (1, 200, 56, 24, 24)\n",
      "    f_out: (1, 100, 52, 20, 20)\n",
      "    number of variables added: 900300, new total: 3874100\n",
      "g_out: (1, 100, 52, 20, 20)\n",
      "g_out_upsampled: (1, 100, 52, 60, 60)\n",
      "f_left_cropped: (1, 100, 52, 60, 60)\n",
      "f_right: (1, 200, 52, 60, 60)\n",
      "f_out: (1, 100, 48, 56, 56)\n",
      "number of variables added: 900300, new total: 4774400\n",
      "input shape : (84, 268, 268)\n",
      "output shape: [48, 56, 56]\n",
      "\n",
      "JAIME!\n"
     ]
    }
   ],
   "source": [
    "network = \"cersei\"\n",
    "create_network(**networks[network])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "## A gunpowder training pipeline\n",
    "\n",
    "In this exercise we ask you to use gunpowder in combination with tensorflow to build a training pipeline for affinity prediction. Fill in the gaps in the template code below and train your network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gunpowder.tensorflow.local_server:Creating local tensorflow server\n",
      "INFO:gunpowder.tensorflow.local_server:Server running at b'grpc://localhost:36565'\n",
      "INFO:gunpowder.tensorflow.nodes.train:Initializing tf session, connecting to b'grpc://localhost:36565'...\n",
      "INFO:gunpowder.tensorflow.nodes.train:Reading meta-graph...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTEXT:  (960, 112, 112)\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gunpowder.tensorflow.nodes.train:No checkpoint found\n",
      "INFO:gunpowder.nodes.precache:starting new set of workers...\n",
      "INFO:gunpowder.producer_pool:terminating workers...\n",
      "INFO:gunpowder.producer_pool:joining workers...\n",
      "INFO:gunpowder.producer_pool:done\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Conv3DBackpropInputOpV2 only supports NDHWC on the CPU.\n\t [[Node: setup0/decoder_0_layer_2/unet_up_3_to_2/conv3d_transpose = Conv3DBackpropInputV2[T=DT_FLOAT, Tshape=DT_INT32, data_format=\"NCDHW\", dilations=[1, 1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 3, 3, 3], _device=\"/job:local/replica:0/task:0/device:CPU:0\"](setup0/gradients/setup0/decoder_0_layer_2/concat_grad/Shape, setup0/decoder_0_layer_2/unet_up_3_to_2/kernel/read, setup0/unet_layer_3_left_1/Relu)]]\n\nCaused by op 'setup0/decoder_0_layer_2/unet_up_3_to_2/conv3d_transpose', defined at:\n  File \"/home/paperspace/anaconda3/envs/segmentation/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/paperspace/anaconda3/envs/segmentation/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/paperspace/anaconda3/envs/segmentation/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/paperspace/anaconda3/envs/segmentation/lib/python3.6/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/home/paperspace/anaconda3/envs/segmentation/lib/python3.6/asyncio/base_events.py\", line 1425, in _run_once\n    handle._run()\n  File \"/home/paperspace/anaconda3/envs/segmentation/lib/python3.6/asyncio/events.py\", line 126, in _run\n    self._callback(*self._args)\n  File \"/home/paperspace/anaconda3/envs/segmentation/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/home/paperspace/anaconda3/envs/segmentation/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/paperspace/anaconda3/envs/segmentation/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/home/paperspace/anaconda3/envs/segmentation/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/paperspace/anaconda3/envs/segmentation/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/paperspace/anaconda3/envs/segmentation/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/paperspace/anaconda3/envs/segmentation/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3049, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3214, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-3-08b68fe0e88e>\", line 168, in <module>\n    train_until(iteration)\n  File \"<ipython-input-3-08b68fe0e88e>\", line 160, in train_until\n    with build(train_pipeline) as b:\n  File \"/home/paperspace/Code/wh2019_segmentation/src/gunpowder/gunpowder/build.py\", line 12, in __enter__\n    self.batch_provider.setup()\n  File \"/home/paperspace/Code/wh2019_segmentation/src/gunpowder/gunpowder/batch_provider_tree.py\", line 17, in setup\n    self.__rec_setup(self.output)\n  File \"/home/paperspace/Code/wh2019_segmentation/src/gunpowder/gunpowder/batch_provider_tree.py\", line 70, in __rec_setup\n    self.__rec_setup(upstream_provider)\n  File \"/home/paperspace/Code/wh2019_segmentation/src/gunpowder/gunpowder/batch_provider_tree.py\", line 70, in __rec_setup\n    self.__rec_setup(upstream_provider)\n  File \"/home/paperspace/Code/wh2019_segmentation/src/gunpowder/gunpowder/batch_provider_tree.py\", line 70, in __rec_setup\n    self.__rec_setup(upstream_provider)\n  File \"/home/paperspace/Code/wh2019_segmentation/src/gunpowder/gunpowder/batch_provider_tree.py\", line 71, in __rec_setup\n    provider.setup()\n  File \"/home/paperspace/Code/wh2019_segmentation/src/gunpowder/gunpowder/nodes/generic_train.py\", line 114, in setup\n    self.start()\n  File \"/home/paperspace/Code/wh2019_segmentation/src/gunpowder/gunpowder/tensorflow/nodes/train.py\", line 163, in start\n    self.__read_meta_graph()\n  File \"/home/paperspace/Code/wh2019_segmentation/src/gunpowder/gunpowder/tensorflow/nodes/train.py\", line 243, in __read_meta_graph\n    clear_devices=True)\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1939, in import_meta_graph\n    **kwargs)\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/tensorflow/python/framework/meta_graph.py\", line 744, in import_scoped_meta_graph\n    producer_op_list=producer_op_list)\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 442, in import_graph_def\n    _ProcessNewOps(graph)\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 234, in _ProcessNewOps\n    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3289, in _add_new_tf_operations\n    for c_op in c_api_util.new_tf_operations(self)\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3289, in <listcomp>\n    for c_op in c_api_util.new_tf_operations(self)\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3180, in _create_op_from_tf_operation\n    ret = Operation(c_op, self)\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): Conv3DBackpropInputOpV2 only supports NDHWC on the CPU.\n\t [[Node: setup0/decoder_0_layer_2/unet_up_3_to_2/conv3d_transpose = Conv3DBackpropInputV2[T=DT_FLOAT, Tshape=DT_INT32, data_format=\"NCDHW\", dilations=[1, 1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 3, 3, 3], _device=\"/job:local/replica:0/task:0/device:CPU:0\"](setup0/gradients/setup0/decoder_0_layer_2/concat_grad/Shape, setup0/decoder_0_layer_2/unet_up_3_to_2/kernel/read, setup0/unet_layer_3_left_1/Relu)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Conv3DBackpropInputOpV2 only supports NDHWC on the CPU.\n\t [[Node: setup0/decoder_0_layer_2/unet_up_3_to_2/conv3d_transpose = Conv3DBackpropInputV2[T=DT_FLOAT, Tshape=DT_INT32, data_format=\"NCDHW\", dilations=[1, 1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 3, 3, 3], _device=\"/job:local/replica:0/task:0/device:CPU:0\"](setup0/gradients/setup0/decoder_0_layer_2/concat_grad/Shape, setup0/decoder_0_layer_2/unet_up_3_to_2/kernel/read, setup0/unet_layer_3_left_1/Relu)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-08b68fe0e88e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0miteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0mtrain_until\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-08b68fe0e88e>\u001b[0m in \u001b[0;36mtrain_until\u001b[0;34m(max_iteration)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pipeline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iteration\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrained_until\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training finished\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/wh2019_segmentation/src/gunpowder/gunpowder/nodes/batch_provider.py\u001b[0m in \u001b[0;36mrequest_batch\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_request_consistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprovide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_batch_consistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/wh2019_segmentation/src/gunpowder/gunpowder/batch_provider_tree.py\u001b[0m in \u001b[0;36mprovide\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You are requesting a batch from an uninitialized provider ('setup()' has not been called). Avoid this by using the 'gunpowder.build' context manager, which also takes care of tearing the provider down if it is not used anymore.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__add__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_provider\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/wh2019_segmentation/src/gunpowder/gunpowder/nodes/batch_provider.py\u001b[0m in \u001b[0;36mrequest_batch\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_request_consistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprovide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_batch_consistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/wh2019_segmentation/src/gunpowder/gunpowder/nodes/batch_filter.py\u001b[0m in \u001b[0;36mprovide\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mtiming_prepare\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_upstream_provider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupstream_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mtiming_process\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTiming\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'process'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/wh2019_segmentation/src/gunpowder/gunpowder/nodes/batch_provider.py\u001b[0m in \u001b[0;36mrequest_batch\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_request_consistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprovide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_batch_consistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/wh2019_segmentation/src/gunpowder/gunpowder/nodes/batch_filter.py\u001b[0m in \u001b[0;36mprovide\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mtiming_prepare\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_upstream_provider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupstream_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mtiming_process\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTiming\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'process'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/wh2019_segmentation/src/gunpowder/gunpowder/nodes/batch_provider.py\u001b[0m in \u001b[0;36mrequest_batch\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_request_consistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprovide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_batch_consistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/wh2019_segmentation/src/gunpowder/gunpowder/nodes/batch_filter.py\u001b[0m in \u001b[0;36mprovide\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mtiming_prepare\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_upstream_provider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupstream_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mtiming_process\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTiming\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'process'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/wh2019_segmentation/src/gunpowder/gunpowder/nodes/batch_provider.py\u001b[0m in \u001b[0;36mrequest_batch\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_request_consistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprovide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_batch_consistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/wh2019_segmentation/src/gunpowder/gunpowder/nodes/batch_filter.py\u001b[0m in \u001b[0;36mprovide\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mtiming_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/wh2019_segmentation/src/gunpowder/gunpowder/nodes/generic_train.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, batch, request)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mtime_of_iteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/wh2019_segmentation/src/gunpowder/gunpowder/tensorflow/nodes/train.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, batch, request)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;31m# compute outputs, gradients, and update variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mto_compute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_compute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1291\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Conv3DBackpropInputOpV2 only supports NDHWC on the CPU.\n\t [[Node: setup0/decoder_0_layer_2/unet_up_3_to_2/conv3d_transpose = Conv3DBackpropInputV2[T=DT_FLOAT, Tshape=DT_INT32, data_format=\"NCDHW\", dilations=[1, 1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 3, 3, 3], _device=\"/job:local/replica:0/task:0/device:CPU:0\"](setup0/gradients/setup0/decoder_0_layer_2/concat_grad/Shape, setup0/decoder_0_layer_2/unet_up_3_to_2/kernel/read, setup0/unet_layer_3_left_1/Relu)]]\n\nCaused by op 'setup0/decoder_0_layer_2/unet_up_3_to_2/conv3d_transpose', defined at:\n  File \"/home/paperspace/anaconda3/envs/segmentation/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/paperspace/anaconda3/envs/segmentation/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/paperspace/anaconda3/envs/segmentation/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/paperspace/anaconda3/envs/segmentation/lib/python3.6/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/home/paperspace/anaconda3/envs/segmentation/lib/python3.6/asyncio/base_events.py\", line 1425, in _run_once\n    handle._run()\n  File \"/home/paperspace/anaconda3/envs/segmentation/lib/python3.6/asyncio/events.py\", line 126, in _run\n    self._callback(*self._args)\n  File \"/home/paperspace/anaconda3/envs/segmentation/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/home/paperspace/anaconda3/envs/segmentation/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/paperspace/anaconda3/envs/segmentation/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/home/paperspace/anaconda3/envs/segmentation/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/paperspace/anaconda3/envs/segmentation/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/paperspace/anaconda3/envs/segmentation/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/paperspace/anaconda3/envs/segmentation/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3049, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3214, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-3-08b68fe0e88e>\", line 168, in <module>\n    train_until(iteration)\n  File \"<ipython-input-3-08b68fe0e88e>\", line 160, in train_until\n    with build(train_pipeline) as b:\n  File \"/home/paperspace/Code/wh2019_segmentation/src/gunpowder/gunpowder/build.py\", line 12, in __enter__\n    self.batch_provider.setup()\n  File \"/home/paperspace/Code/wh2019_segmentation/src/gunpowder/gunpowder/batch_provider_tree.py\", line 17, in setup\n    self.__rec_setup(self.output)\n  File \"/home/paperspace/Code/wh2019_segmentation/src/gunpowder/gunpowder/batch_provider_tree.py\", line 70, in __rec_setup\n    self.__rec_setup(upstream_provider)\n  File \"/home/paperspace/Code/wh2019_segmentation/src/gunpowder/gunpowder/batch_provider_tree.py\", line 70, in __rec_setup\n    self.__rec_setup(upstream_provider)\n  File \"/home/paperspace/Code/wh2019_segmentation/src/gunpowder/gunpowder/batch_provider_tree.py\", line 70, in __rec_setup\n    self.__rec_setup(upstream_provider)\n  File \"/home/paperspace/Code/wh2019_segmentation/src/gunpowder/gunpowder/batch_provider_tree.py\", line 71, in __rec_setup\n    provider.setup()\n  File \"/home/paperspace/Code/wh2019_segmentation/src/gunpowder/gunpowder/nodes/generic_train.py\", line 114, in setup\n    self.start()\n  File \"/home/paperspace/Code/wh2019_segmentation/src/gunpowder/gunpowder/tensorflow/nodes/train.py\", line 163, in start\n    self.__read_meta_graph()\n  File \"/home/paperspace/Code/wh2019_segmentation/src/gunpowder/gunpowder/tensorflow/nodes/train.py\", line 243, in __read_meta_graph\n    clear_devices=True)\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1939, in import_meta_graph\n    **kwargs)\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/tensorflow/python/framework/meta_graph.py\", line 744, in import_scoped_meta_graph\n    producer_op_list=producer_op_list)\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 442, in import_graph_def\n    _ProcessNewOps(graph)\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 234, in _ProcessNewOps\n    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3289, in _add_new_tf_operations\n    for c_op in c_api_util.new_tf_operations(self)\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3289, in <listcomp>\n    for c_op in c_api_util.new_tf_operations(self)\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3180, in _create_op_from_tf_operation\n    ret = Operation(c_op, self)\n  File \"/home/paperspace/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): Conv3DBackpropInputOpV2 only supports NDHWC on the CPU.\n\t [[Node: setup0/decoder_0_layer_2/unet_up_3_to_2/conv3d_transpose = Conv3DBackpropInputV2[T=DT_FLOAT, Tshape=DT_INT32, data_format=\"NCDHW\", dilations=[1, 1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 3, 3, 3], _device=\"/job:local/replica:0/task:0/device:CPU:0\"](setup0/gradients/setup0/decoder_0_layer_2/concat_grad/Shape, setup0/decoder_0_layer_2/unet_up_3_to_2/kernel/read, setup0/unet_layer_3_left_1/Relu)]]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "from gunpowder import *\n",
    "from gunpowder.tensorflow import *\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "data_dir = '../../jan/segmentation/data'\n",
    "\n",
    "samples = [\n",
    "    'sample_A',\n",
    "    'sample_B',\n",
    "    'sample_C'\n",
    "]\n",
    "\n",
    "neighborhood = [[-1, 0, 0], [0, -1, 0], [0, 0, -1]]\n",
    "\n",
    "def train_until(max_iteration):\n",
    "\n",
    "    if tf.train.latest_checkpoint('.'):\n",
    "        trained_until = int(tf.train.latest_checkpoint('.').split('_')[-1])\n",
    "    else:\n",
    "        trained_until = 0\n",
    "    if trained_until >= max_iteration:\n",
    "        return\n",
    "\n",
    "    with open('train_net.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    raw = ArrayKey('RAW')\n",
    "    labels = ArrayKey('GT_LABELS')\n",
    "    labels_mask = ArrayKey('GT_LABELS_MASK')\n",
    "    affs = ArrayKey('PREDICTED_AFFS')\n",
    "    gt = ArrayKey('GT_AFFINITIES')\n",
    "    gt_mask = ArrayKey('GT_AFFINITIES_MASK')\n",
    "    gt_scale = ArrayKey('GT_AFFINITIES_SCALE')\n",
    "    affs_gradient = ArrayKey('AFFS_GRADIENT')\n",
    "\n",
    "    voxel_size = Coordinate((40, 4, 4))\n",
    "    input_size = Coordinate(config['input_shape'])*voxel_size\n",
    "    output_size = Coordinate(config['output_shape'])*voxel_size\n",
    "    context = output_size/2\n",
    "    print('CONTEXT: ', context)\n",
    "\n",
    "    request = BatchRequest()\n",
    "    request.add(raw, input_size)\n",
    "    request.add(labels, output_size)\n",
    "    request.add(labels_mask, output_size)\n",
    "    request.add(gt, output_size)\n",
    "    request.add(gt_mask, output_size)\n",
    "    request.add(gt_scale, output_size)\n",
    "\n",
    "    snapshot_request = BatchRequest({\n",
    "        affs: request[gt],\n",
    "        affs_gradient: request[gt]\n",
    "    })\n",
    "\n",
    "    data_sources = tuple(\n",
    "        N5Source(\n",
    "            os.path.join(data_dir, sample + '.n5'),\n",
    "            datasets = {\n",
    "                raw: 'volumes/raw',\n",
    "                labels: 'volumes/labels/neuron_ids',\n",
    "                labels_mask: 'volumes/labels/mask',\n",
    "            },\n",
    "            array_specs = {\n",
    "                raw: ArraySpec(interpolatable=True),\n",
    "                labels: ArraySpec(interpolatable=False),\n",
    "                labels_mask: ArraySpec(interpolatable=False)\n",
    "            }\n",
    "        ) +\n",
    "        Normalize(raw) +\n",
    "        Pad(labels, context) +\n",
    "        Pad(labels_mask, context) +\n",
    "        RandomLocation() +\n",
    "        Reject(mask=labels_mask)\n",
    "        for sample in samples\n",
    "    )\n",
    "\n",
    "\n",
    "    train_pipeline = (\n",
    "        data_sources +\n",
    "        RandomProvider() +\n",
    "        ElasticAugment(\n",
    "            control_point_spacing=[4,40,40],\n",
    "            jitter_sigma=[0,2,2],\n",
    "            rotation_interval=[0,math.pi/2.0],\n",
    "            prob_slip=0.05,\n",
    "            prob_shift=0.05,\n",
    "            max_misalign=10,\n",
    "            subsample=8) +\n",
    "        SimpleAugment(transpose_only=[1, 2]) +\n",
    "        IntensityAugment(raw, 0.9, 1.1, -0.1, 0.1, z_section_wise=True) +\n",
    "        GrowBoundary(labels, labels_mask, steps=1, only_xy=True) +\n",
    "        AddAffinities(\n",
    "            neighborhood,\n",
    "            labels=labels,\n",
    "            affinities=gt,\n",
    "            labels_mask=labels_mask,\n",
    "            affinities_mask=gt_mask) +\n",
    "        BalanceLabels(\n",
    "            gt,\n",
    "            gt_scale,\n",
    "            gt_mask) +\n",
    "        DefectAugment(\n",
    "            raw,\n",
    "            prob_missing=0.03,\n",
    "            prob_low_contrast=0.01,\n",
    "            contrast_scale=0.5,\n",
    "            axis=0) +\n",
    "        IntensityScaleShift(raw, 2,-1) +\n",
    "        PreCache(cache_size=40,\n",
    "                 num_workers=10) +\n",
    "        Train(\n",
    "            'train_net',\n",
    "            optimizer=config['optimizer'],\n",
    "            loss=config['loss'],\n",
    "            inputs={\n",
    "                config['raw']: raw,\n",
    "                config['gt_affs']: gt,\n",
    "                config['affs_loss_weights']: gt_scale,\n",
    "            },\n",
    "            outputs={\n",
    "                config['affs']: affs\n",
    "            },\n",
    "            gradients={\n",
    "                config['affs']: affs_gradient\n",
    "            },\n",
    "            summary=config['summary'],\n",
    "            log_dir='log',\n",
    "            save_every=10000) +\n",
    "        IntensityScaleShift(raw, 0.5, 0.5) +\n",
    "        Snapshot({\n",
    "                raw: 'volumes/raw',\n",
    "                labels: 'volumes/labels/neuron_ids',\n",
    "                gt: 'volumes/gt_affinities',\n",
    "                affs: 'volumes/pred_affinities',\n",
    "                gt_mask: 'volumes/labels/gt_mask',\n",
    "                labels_mask: 'volumes/labels/mask',\n",
    "                affs_gradient: 'volumes/affs_gradient'\n",
    "            },\n",
    "            dataset_dtypes={\n",
    "                labels: np.uint64\n",
    "            },\n",
    "            every=1000,\n",
    "            output_filename='batch_{iteration}.hdf',\n",
    "            additional_request=snapshot_request) +\n",
    "        PrintProfilingStats(every=10)\n",
    "    )\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    with build(train_pipeline) as b:\n",
    "        for i in range(max_iteration - trained_until):\n",
    "            b.request_batch(request)\n",
    "    print(\"Training finished\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    iteration = 500000\n",
    "    train_until(iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "## Monitor the training progress\n",
    "\n",
    "Visualize the generated training snapshots with neuroglancer and observe loss curves using tensorboard.\n",
    "\n",
    "\n",
    "#### Snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuroglancer\n",
    "import funlib.show.neuroglancer as funlibng\n",
    "import daisy\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "data_file = ...\n",
    "\"\"\"\n",
    "\n",
    "gt_affs_ds = daisy.open_ds(data_file, 'volumes/gt_affinities')\n",
    "gt_affs_ds.data = np.array(gt_affs_ds.data, dtype=np.float32)\n",
    "\n",
    "pred_affs_ds = daisy.open_ds(data_file, 'volumes/pred_affinities')\n",
    "pred_affs_ds.data = np.array(pred_affs_ds.data, dtype=np.float32)\n",
    "\n",
    "gradients_ds = daisy.open_ds(data_file, 'volumes/affs_gradient')\n",
    "\n",
    "labels_ds = daisy.open_ds(data_file, 'volumes/labels/neuron_ids')\n",
    "raw_ds = daisy.open_ds(data_file, 'volumes/raw')\n",
    "\n",
    "# Replace my_ip with your public paperspace ip as shown in your browser\n",
    "my_ip = \"172.83.14.204\"\n",
    "neuroglancer.set_server_bind_address('0.0.0.0', 8889)\n",
    "\n",
    "viewer = neuroglancer.Viewer()\n",
    "with viewer.txn() as s:\n",
    "    funlibng.add_layer(s, raw_ds, 'raw')\n",
    "    funlibng.add_layer(s, labels_ds, 'labels')\n",
    "    funlibng.add_layer(s, gt_affs_ds, 'gt_affinities', shader='rgb')\n",
    "    funlibng.add_layer(s, pred_affs_ds, 'pred_affinities', shader='rgb')\n",
    "    funlibng.add_layer(s, gradients_ds, 'gradients')\n",
    "\n",
    "print(viewer.__str__().replace(\"localhost\", my_ip))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss\n",
    "To visualize your loss using tensorboard open a terminal and execute: \n",
    "```tensorboard --logdir=path/to/log-directory```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:segmentation]",
   "language": "python",
   "name": "conda-env-segmentation-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
