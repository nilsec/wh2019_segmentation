{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Affinities to Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Load the inference results\n",
    "1. make sure you predicted affinities with `02_inference.ipynb`\n",
    "2. You have two different datasetsizes written out, examples are nicer with datasetsize=big, but if the functions are too slow for you, consider to switch to datasetsize=small\n",
    "3. Don't forget to set the variable `ip_address` in this notebook (this is necassary for 3D visualization in neuroglancer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your ip address here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ip_address = 'localhost'    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Help & Troubleshooting\n",
    "- If your notebook is hanging, sometimes it helps to restart the kernel: For this go to `Kernel` and then `Restart & Clear Output`\n",
    "- We experience some issues when running multiple `neuroglancer` instances at the same time. Make sure you close the tab of one `neuroglancer` instance before you run the next `neuroglancer` cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import daisy\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import ndimage\n",
    "import funlib.segment\n",
    "import malis\n",
    "import matplotlib.gridspec as gridspec\n",
    "import waterz\n",
    "import logging\n",
    "from importlib import reload\n",
    "import utils\n",
    "import funlib.show.neuroglancer as funlibng\n",
    "import neuroglancer\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data (make sure you ran inference.ipynb before).\n",
    "# By default, you wrote out the big ROI at iteration 500000. However, if you changed iteration \n",
    "# or datasetsize, model or out_file name\n",
    "datasetsize = 'small' # change accordingly\n",
    "iteration = 500000 # Change to check out other training iterations\n",
    "aff_file = 'affinities_{}_{:05}.zarr'.format(datasetsize, iteration)\n",
    "\n",
    "\n",
    "\n",
    "# --------- Do Not Change --------\n",
    "# gt_file = '../../jan/segmentation/data/sample_0.n5'\n",
    "gt_file =  'sample_0.n5'\n",
    "affs_ds = daisy.open_ds(aff_file, 'volumes/affs')\n",
    "raw_ds = daisy.open_ds(aff_file, 'volumes/raw')\n",
    "gtds = daisy.open_ds(gt_file, 'volumes/labels/neuron_ids')\n",
    "\n",
    "roi = affs_ds.roi.intersect(gtds.roi) # The Region of Interest that we will look at in this notebook\n",
    "\n",
    "gt = gtds.to_ndarray(roi)\n",
    "raw = raw_ds.to_ndarray(roi)\n",
    "affs = affs_ds.to_ndarray(roi)\n",
    "\n",
    "z_section = 9 # For 2d Visualization, always pick the same z_section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Affinities are in z,y,x direction, here average over x and y vor visualization.\n",
    "mean_affs = 0.5*(affs[1] + affs[2]) \n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "gs = gridspec.GridSpec(1, 3)\n",
    "\n",
    "ax = plt.subplot(gs[0,0])\n",
    "plt.imshow(np.squeeze(mean_affs[z_section, :, :]), cmap='gray')\n",
    "ax.axis('off')  # clear x- and y-axes\n",
    "plt.title('Predicted Affinities')\n",
    "\n",
    "ax = plt.subplot(gs[0,1])\n",
    "ax.imshow(raw[z_section, :], cmap='gray')\n",
    "ax.axis('off')  # clear x- and y-axes\n",
    "plt.title('Raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain a segmentation by thresholding\n",
    "Given a perfect boundary prediction, obtaining a segmentation would be trivial. It would be enough to create a binary mask by thresholding and then finding connected component. As we will see, minor errors in our boundary predictions can lead to catastropical errors in our segmentation\n",
    "### 2D example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def connected_component_sectionwise(affs, threshold):\n",
    "    mean_affs = 0.5*(affs[1] + affs[2])\n",
    "    depth = mean_affs.shape[0]\n",
    "    cc = np.zeros(mean_affs.shape, dtype=np.uint64)\n",
    "    boundary = np.zeros(mean_affs.shape, dtype=np.uint8)\n",
    "    for z in range(depth):\n",
    "        boundary_mask = mean_affs[z] > 0.5\n",
    "\n",
    "        boundarysingle = mean_affs[z, :] > threshold\n",
    "        ccsingle, num_labels = ndimage.label(boundarysingle)\n",
    "        cc[z] = ccsingle\n",
    "        boundary[z] = boundarysingle\n",
    "    return cc, boundary\n",
    "\n",
    "threshold = 0.3\n",
    "cc, boundary = connected_component_sectionwise(affs, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "gs = gridspec.GridSpec(1, 3)\n",
    "\n",
    "ax = plt.subplot(gs[0,0])\n",
    "ax.imshow(boundary[z_section, :], cmap='gray')\n",
    "ax.axis('off')  # clear x- and y-axes\n",
    "plt.title('Binary')\n",
    "\n",
    "ax = plt.subplot(gs[0,1])\n",
    "ax.imshow(cc[z_section, :], cmap='viridis')\n",
    "ax.axis('off')  # clear x- and y-axes\n",
    "plt.title('ConnectedComponents')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the snapshots, to get a good impression, it is better to explore the data in 3D with neuroglancer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neuroglancer.set_server_bind_address('0.0.0.0')\n",
    "viewer = neuroglancer.Viewer()\n",
    "with viewer.txn() as s:\n",
    "    funlibng.add_layer(s, raw_ds, 'raw')\n",
    "    funlibng.add_layer(s, affs_ds, 'affs', shader='rgb')\n",
    "    funlibng.add_layer(s, daisy.Array(cc, roi, affs_ds.voxel_size), 'connected components')\n",
    "    funlibng.add_layer(s, daisy.Array(boundary, roi, affs_ds.voxel_size), 'Binary')\n",
    "if len(ip_address) == 0:\n",
    "    print('you first have to set the ip address of your paperspace machine at the top')\n",
    "else:\n",
    "    print(viewer.__str__().replace('localhost', ip_address))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 : Connected Component in 2D\n",
    "1. Try out connected components for different thresholds and explore the effect in neuroglancer.\n",
    "2. For this, use the function `connected_component_sectionwise` of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Here you should type your solution\n",
    "# ......\n",
    "neuroglancer.set_server_bind_address('0.0.0.0')\n",
    "viewer = neuroglancer.Viewer()\n",
    "with viewer.txn() as s:\n",
    "    funlibng.add_layer(s, raw_ds, 'raw')\n",
    "    funlibng.add_layer(s, affs_ds, 'affs', shader='rgb')\n",
    "    \n",
    "    thresholds = [0.0, 0.3, 0.9, 0.95, 0.99, 1.]\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        cc, boundary = connected_component_sectionwise(affs, threshold)\n",
    "        funlibng.add_layer(s, daisy.Array(cc, roi, affs_ds.voxel_size), 'cc_{:0.2f}'.format(threshold))\n",
    "        funlibng.add_layer(s, daisy.Array(boundary, roi, affs_ds.voxel_size), 'boundary_{:0.2f}'.format(threshold))\n",
    "    \n",
    "    \n",
    "if len(ip_address) == 0:\n",
    "    print('you first have to set the ip address of your paperspace machine at the top')\n",
    "else:\n",
    "    print(viewer.__str__().replace('localhost', ip_address))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D example\n",
    "The 2D example already shows, that some neurons accidently gets merged, when there is a small hole in the membrane. This gets worse, if you do the same procedure in 3D.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nhood = [[-1, 0, 0], [0, -1, 0], [0, 0, -1]]\n",
    "cc_3d, __ = malis.connected_components_affgraph(np.array(affs > threshold, dtype=np.int32),np.array(nhood))\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "\n",
    "im = ax.imshow(cc_3d[z_section, :], cmap='viridis')\n",
    "ax.axis('off')  # clear x- and y-axes\n",
    "plt.title('Connected Component from Affinity Graph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neuroglancer.set_server_bind_address('0.0.0.0')\n",
    "viewer = neuroglancer.Viewer()\n",
    "with viewer.txn() as s:\n",
    "    funlibng.add_layer(s, raw_ds, 'raw')\n",
    "    funlibng.add_layer(s, affs_ds, 'affs', shader='rgb')\n",
    "    funlibng.add_layer(s, daisy.Array(cc_3d, roi, affs_ds.voxel_size), 'connected components')\n",
    "print(viewer.__str__().replace('localhost', ip_address))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Watershed and Fragment Agglomeration for a more robust segmentation\n",
    "## Creation of Fragments\n",
    "We use the watershed algorithm to create an oversegmentation. The Watershed is more robust towards small errors in the hole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fragments = utils.apply_watershed(affs, fragments_in_xy=True)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "gs = gridspec.GridSpec(1, 3)\n",
    "\n",
    "ax = plt.subplot(gs[0,0])\n",
    "plt.imshow(fragments[z_section, :], cmap='viridis')\n",
    "ax.axis('off')  # clear x- and y-axes\n",
    "plt.title('Initial Fragments')\n",
    "\n",
    "ax = plt.subplot(gs[0,1])\n",
    "ax.imshow(raw[z_section, :], cmap='gray')\n",
    "ax.axis('off')  # clear x- and y-axes\n",
    "plt.title('Raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomeration\n",
    "The watershed is good to prevent too much merging. However, we can clearly see, that now the neurons are broken into too many pieces. In this part, we will see, how we can agglomerate those little pieces / fragments into larger ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "threshold = 0.99\n",
    "segments = utils.agglomerate_fragments(affs, fragments, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "gs = gridspec.GridSpec(1, 3)\n",
    "\n",
    "ax = plt.subplot(gs[0,0])\n",
    "plt.imshow(np.array(segments[z_section, :], dtype=np.float32), cmap='viridis')\n",
    "ax.axis('off')  # clear x- and y-axes\n",
    "plt.title('Segments')\n",
    "\n",
    "ax = plt.subplot(gs[0,1])\n",
    "ax.imshow(raw[z_section, :], cmap='gray')\n",
    "ax.axis('off')  # clear x- and y-axes\n",
    "plt.title('Raw')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: How does the segmentation change when we change the agglomeration threshold ?\n",
    "1. Try to find a threshold, that produces meaningful results (this means, ideally, no two neurons should be merged together, and not a single neuron should be split into two segments\n",
    "\n",
    "2. We also add our ground truth data here, such that you see, what we are aiming for "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Here you should type your solution\n",
    "# 1) apply agglomeration functions for different thresholds (utils.agglomerate_fragments)\n",
    "# 2) see the result in neuroglancer\n",
    "neuroglancer.set_server_bind_address('0.0.0.0')\n",
    "viewer = neuroglancer.Viewer()\n",
    "with viewer.txn() as s:\n",
    "    funlibng.add_layer(s, raw_ds, 'raw')\n",
    "    funlibng.add_layer(s, affs_ds, 'pred_affs', shader='rgb')\n",
    "    funlibng.add_layer(s, daisy.Array(fragments, roi, affs_ds.voxel_size), 'Fragments')\n",
    "    funlibng.add_layer(s, daisy.Array(segments, roi, affs_ds.voxel_size), 'Segmentation')\n",
    "    funlibng.add_layer(s, daisy.Array(gt, roi, affs_ds.voxel_size), 'Ground Truth')\n",
    "    # Add here more segmentation with different thresholds; for instance:\n",
    "    # funlibng.add_layer(s, daisy.Array(segment_thr05, roi, affs_ds.voxel_size), 'Segmentation')\n",
    "    thresholds = [0.3, 0.9, 0.95, 0.99, 1.]\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        segments = utils.agglomerate_fragments(affs, fragments, threshold)\n",
    "        funlibng.add_layer(s, daisy.Array(segments, roi, affs_ds.voxel_size), 'segmentation_{:0.2f}'.format(threshold))\n",
    "\n",
    "    print(viewer.__str__().replace('localhost', ip_address))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neuroglancer.set_server_bind_address('0.0.0.0')\n",
    "viewer = neuroglancer.Viewer()\n",
    "with viewer.txn() as s:\n",
    "    funlibng.add_layer(s, raw_ds, 'raw')\n",
    "    funlibng.add_layer(s, affs_ds, 'pred_affs', shader='rgb')\n",
    "    funlibng.add_layer(s, daisy.Array(fragments, roi, affs_ds.voxel_size), 'Fragments')\n",
    "    funlibng.add_layer(s, daisy.Array(segments, roi, affs_ds.voxel_size), 'Segmentation')\n",
    "    funlibng.add_layer(s, daisy.Array(gt, roi, affs_ds.voxel_size), 'Ground Truth')\n",
    "    # Add here more segmentation with different thresholds; for instance:\n",
    "    # funlibng.add_layer(s, daisy.Array(segment_thr05, roi, affs_ds.voxel_size), 'Segmentation')\n",
    "\n",
    "    print(viewer.__str__().replace('localhost', ip_address))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation (Extra Exercises)\n",
    "So far, we only assessed qualitatively our results. Now, we will have a look how we can quantitavely check the performance our of our models. For this, we use our ground truth data to compare it to our predicted segmentation,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gt, __ = funlib.segment.arrays.relabel(gt, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 22})\n",
    "plt.figure(figsize=(20, 20))\n",
    "gs = gridspec.GridSpec(1, 3)\n",
    "\n",
    "ax = plt.subplot(gs[0,0])\n",
    "plt.imshow(np.squeeze(affs[0, z_section, :]), cmap='gray')\n",
    "ax.set_title('Affinities')\n",
    "ax.axis('off') \n",
    "ax = plt.subplot(gs[0,1])\n",
    "ax.imshow(np.squeeze(gt[z_section, :]), cmap='viridis')\n",
    "ax.set_title('Ground Truth')\n",
    "ax.axis('off') \n",
    "ax = plt.subplot(gs[0,2])\n",
    "ax.imshow(segments[z_section, :], cmap='viridis')\n",
    "ax.set_title('Predicted Segmentation')\n",
    "ax.axis('off') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `VOI` to measure performance of our neuron segmentation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(prediction, groundtruth):\n",
    "    \"\"\"Evaluates a segmentation with a ground truth segmentation.\n",
    "    \n",
    "    Args:\n",
    "        prediction (np.array): The predicted segmentation.\n",
    "        groundtruth (np.array): The ground truth segmentation\n",
    "        \n",
    "    Returns:\n",
    "        voi_split: (float): VOI split\n",
    "        voi_merge: (float): VOI merge\n",
    "    \"\"\"\n",
    "    metric = waterz.evaluate(prediction, groundtruth)\n",
    "    return metric['voi_split'], metric['voi_merge']\n",
    "\n",
    "# Example usage:\n",
    "voi_split, voi_merge = evaluate(segments, gt)\n",
    "print(voi_split, voi_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: How does VOI_split and VOI_merge change with changing agglomeration thresholds ?\n",
    "In this exercise you should\n",
    "\n",
    "  1. create multiple segmentations for different thresholds\n",
    "  2. Evalute each segmentation against the ground truth segmentation (with the function above: `evaluate`)\n",
    "  3. Make a a) plot with the threshold plotted against VOI_split and b) a plot with the threshold plotted against VOI_merge\n",
    "  4. Take the threshold that minimizes the sum of VOI_split and VOI_merge and look at the corresponding segmentation in neuroglancer (go back to exercise 2 and plug in your selected threshold\n",
    "  5. Check out the [CREMI benchmark](www.cremi.org) and see what state of the art results are and put your numbers into context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here you should type your solution\n",
    "# ......\n",
    "voi_splits = []\n",
    "voi_merges = []\n",
    "thresholds = [0.1, 0.3, 0.6, 0.9, 0.95, 0.99, 1.]\n",
    "for threshold in thresholds:\n",
    "    segments = utils.agglomerate_fragments(affs, fragments, threshold)\n",
    "    voi_split, voi_merge = evaluate(segments, gt)\n",
    "    voi_splits.append(voi_split)\n",
    "    voi_merges.append(voi_merge)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(thresholds, voi_splits, label='VOI_splits')\n",
    "plt.plot(thresholds, voi_merges, label='VOI_merges')\n",
    "plt.title('Neuron segmentation performance (VOI) for iteration {}'.format(iteration))\n",
    "plt.legend()\n",
    "\n",
    "best_index = np.argmax((np.array(voi_splits)+np.array(voi_merges)))\n",
    "print('threshold at which sum of VOI_split and VOI_merge is best: {}'.format(thresholds[best_index]))\n",
    "\n",
    "print(np.argmin((np.array(voi_splits)+np.array(voi_merges))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 (extra, extra, extra :)) : Does the same for different checkpoints\n",
    "eg. how do those numbers evolve during training, eg. at 30k, 60k ?\n",
    "In this exercise you should\n",
    "\n",
    "  1. create multiple segmentations for different iterations\n",
    "  2. Evalute each segmentation against the ground truth segmentation\n",
    "  3. Make a a) plot with the iteration plotted against VOI_split and b) a plot with the iteration plotted against VOI_merge\n",
    "  4. Check out the [CREMI benchmark](www.cremi.org) and see what state of the art results are"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:segmentation]",
   "language": "python",
   "name": "conda-env-segmentation-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
