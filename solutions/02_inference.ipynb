{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "As for training, we use `gunpowder` for predicting a larger area.\n",
    "## Exercise 1 : Start Inference\n",
    "1. Stop first training in `01_training.ipynb`\n",
    "2. Your task is to run the script twice, one time with `datasetsize` set to small, and one to big. With this we get two affinity predictions that we can play with in `03_agglomeration.ipynb`\n",
    "2. The default model that we use to make our prediction is a pretrained network. Change the `setup_dir`, such that it points to your training directory. By doing that, the network's weights are loaded from your model instead from our pretrained model. You also need to adapt `iteration` depending on what the last tensorflow checkpoint was, that are written out. For this go to your training directory, and search for the file with the highest number.\n",
    "\n",
    "Tip: It takes a while for the script to start a prediction. After aorund half a minute, you should be able to see following output. If you wait 2mins and nothing happens, restart the kernel and try again.\n",
    "```\n",
    "INFO:gunpowder.nodes.scan:processed chunk 1/288\n",
    "INFO:gunpowder.nodes.scan:processed chunk 2/288\n",
    "INFO:gunpowder.nodes.scan:processed chunk 3/288\n",
    "INFO:gunpowder.nodes.scan:processed chunk 4/288\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from gunpowder import *\n",
    "from gunpowder.tensorflow import *\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import daisy\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "def predict(iteration, in_file, out_file, setup_dir,\n",
    "            out_dataset, datasetsize='small', networkname='train_net'):\n",
    "\n",
    "    with open(os.path.join(setup_dir, '{}.json'.format(networkname)), 'r') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    # voxels\n",
    "    voxel_size = Coordinate((40, 4, 4))\n",
    "    input_shape = Coordinate(config['input_shape']) # Network specific\n",
    "    output_shape = Coordinate(config['output_shape'])\n",
    "    context = (input_shape - output_shape) // 2\n",
    "    \n",
    "    # Initial ROI\n",
    "    # got nice offset visually from neuroglancer : 1507, 1678, 100\n",
    "    if datasetsize == 'big':\n",
    "        offset = (50, 1400, 1400) * np.array(voxel_size)  # avoid big glia and cell body in sample 0\n",
    "        chunkgrid = [2, 12, 12]\n",
    "\n",
    "    elif datasetsize == 'small':\n",
    "        offset = (90, 1600, 1400) * np.array(voxel_size)  \n",
    "        chunkgrid = [1, 5, 5]\n",
    "\n",
    "\n",
    "    roi = Roi(\n",
    "        offset=offset,\n",
    "        shape=output_shape * voxel_size * Coordinate(chunkgrid),\n",
    "    )\n",
    "\n",
    "\n",
    "    # nm\n",
    "    context_nm = context * voxel_size\n",
    "    read_roi = roi.copy()\n",
    "    read_roi = read_roi.grow(context_nm, context_nm)\n",
    "\n",
    "    input_size = input_shape * voxel_size\n",
    "    output_size = output_shape * voxel_size\n",
    "\n",
    "    output_roi = read_roi.grow(-context_nm, -context_nm)\n",
    "    print(\"Read ROI in nm is %s\" % read_roi)\n",
    "    print(\"Output ROI in nm is %s\" % output_roi)\n",
    "\n",
    "    print(\"Read ROI in voxel space is {}\".format(read_roi / voxel_size))\n",
    "    print(\"Output ROI in voxel space is {}\".format(output_roi / voxel_size))\n",
    "\n",
    "    raw = ArrayKey('RAW')\n",
    "    affs = ArrayKey('AFFS')\n",
    "\n",
    "    output_roi = daisy.Roi(\n",
    "        output_roi.get_begin(),\n",
    "        output_roi.get_shape()\n",
    "    )\n",
    "\n",
    "\n",
    "    # Caution: daisy.ROI and gunpowder.Roi have different behaviour, Prepare_ds only works with daisy.Roi, while\n",
    "    # gunpowder node eg. Crop Node only works with gunpowder.Roi\n",
    "    ds = daisy.prepare_ds(\n",
    "        out_file,\n",
    "        out_dataset,\n",
    "        output_roi,\n",
    "        voxel_size,\n",
    "        'float32',\n",
    "        # write_size=output_size,\n",
    "        write_roi=daisy.Roi((0, 0, 0), output_size),\n",
    "        num_channels=3,\n",
    "        # temporary fix until\n",
    "        # https://github.com/zarr-developers/numcodecs/pull/87 gets approved\n",
    "        # (we want gzip to be the default)\n",
    "        compressor={'id': 'gzip', 'level': 5}\n",
    "    )\n",
    "\n",
    "    chunk_request = BatchRequest()\n",
    "    chunk_request.add(raw, input_size)\n",
    "    chunk_request.add(affs, output_size)\n",
    "\n",
    "    pipeline = (\n",
    "            N5Source(\n",
    "                in_file,\n",
    "                datasets={\n",
    "                    raw: 'volumes/raw'\n",
    "                },\n",
    "            ) +\n",
    "            Pad(raw, size=None) +\n",
    "            Crop(raw, read_roi) +\n",
    "            Normalize(raw) +\n",
    "            IntensityScaleShift(raw, 2, -1) +\n",
    "            Predict(\n",
    "                os.path.join(setup_dir, '%s_checkpoint_%d' % (networkname, iteration)),\n",
    "                inputs={\n",
    "                    config['raw']: raw\n",
    "                },\n",
    "                outputs={\n",
    "                    config['affs']: affs\n",
    "                },\n",
    "                # TODO: change to predict graph\n",
    "                graph=os.path.join(setup_dir, '{}.meta'.format(networkname))\n",
    "            ) +\n",
    "            IntensityScaleShift(raw, 0.5, 0.5) +  # Just for visualization.\n",
    "            ZarrWrite(\n",
    "                dataset_names={\n",
    "                    affs: out_dataset,\n",
    "                    raw: 'volumes/raw',\n",
    "                },\n",
    "                output_filename=out_file\n",
    "            ) +  # TODO: Would be nice to have a consistent file format (eg. only n5)\n",
    "            PrintProfilingStats(every=10) +\n",
    "            Scan(chunk_request)\n",
    "    )\n",
    "    start_time = time.time()\n",
    "    print(\"Starting prediction...\")\n",
    "    with build(pipeline):\n",
    "        pipeline.request_batch(BatchRequest())\n",
    "    print(\"Prediction finished in {:0.2f}\".format(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger('gunpowder.nodes.hdf5like_write_base').setLevel(\n",
    "    logging.INFO)\n",
    "\n",
    "in_file = '../../jan/segmentation/data/sample_0.n5'  # This is our raw file\n",
    "\n",
    "# Baseline Model\n",
    "setup_dir = '../../jan/segmentation/snapshots/setup58_p/' # This is the pretrained model. \n",
    "model = 'train_net'\n",
    "\n",
    "# For your own model\n",
    "setup_dir = '.'\n",
    "model='arya'\n",
    "\n",
    "out_dataset = 'volumes/affs'\n",
    "datasetsize = 'small' # choose big or small\n",
    "iteration = 10\n",
    "\n",
    "out_file = '{}_{}_{:05}.zarr'.format(model, datasetsize, iteration)\n",
    "\n",
    "predict(\n",
    "    iteration,\n",
    "    in_file,\n",
    "    out_file,\n",
    "    setup_dir,\n",
    "    out_dataset,\n",
    "    datasetsize=datasetsize,\n",
    "    networkname=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 : Throughput calculations\n",
    "1. While you are waiting for the prediction to finish, you can calculate how many chunks needed to be predicted, if we were to process an entire fly brain\n",
    "  - the total size of the predicted dataset=big --> (3840, 2688, 2688) nm\n",
    "  - this translates to 288 chunks\n",
    "  - the dimension of the FAFB, a full adult fly brain dataset are: (282, 519, 991) microns\n",
    "\n",
    "2. After the prediction, you can make an estimate for how long it would take, if we only had a single gpu available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your solution here\n",
    "# -------\n",
    "fafbsize = 282000*519000*991000\n",
    "blocksize = (3840*2688*2688)\n",
    "factor = fafbsize/blocksize\n",
    "duration = 322\n",
    "\n",
    "print('Number of blocks to be predicted {}'.format(int(factor)))\n",
    "print('Prediction time {} in days'.format(duration*factor/100./60./24.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 : Check predicted affinities in neuroglancer\n",
    "1. Check out whether your prediction was successfull by exploring the result in `neuroglancer`\n",
    "2. You can predict datasets for multiple iterations, and compare them in neuroglancer\n",
    "3. If you are satisfied, you can move on to `03_agglomeration.ipynb`. If you have the feeling, your predictions are not correct (eg. only black), or you cant see any neuron border, consider to use the pretrained network instead of your own model. You need ok-affinity predictions for the next notebook.\n",
    "4. You might want to come back to this notebook, if you are interested in comparing eg. your model and our pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuroglancer\n",
    "import funlib.show.neuroglancer as funlibng\n",
    "\n",
    "ip_address = '' # Put your ip adress here\n",
    "neuroglancer.set_server_bind_address('0.0.0.0')\n",
    "viewer = neuroglancer.Viewer()\n",
    "\n",
    "raw_ds = daisy.open_ds(in_file, 'volumes/raw')\n",
    "affs_ds = daisy.open_ds(out_file, out_dataset)\n",
    "\n",
    "with viewer.txn() as s:\n",
    "    funlibng.add_layer(s, raw_ds, 'raw')\n",
    "    funlibng.add_layer(s, affs_ds, 'affs', shader='rgb')\n",
    "if len(ip_address) == 0:\n",
    "    print('you first have to set the ip address of your paperspace machine')\n",
    "else:\n",
    "    print(viewer.__str__().replace('localhost', ip_address))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuroglancer\n",
    "import funlib.show.neuroglancer as funlibng\n",
    "\n",
    "ip_address = '' # Put your ip adress here\n",
    "neuroglancer.set_server_bind_address('0.0.0.0')\n",
    "viewer = neuroglancer.Viewer()\n",
    "\n",
    "raw_ds = daisy.open_ds(in_file, 'volumes/raw')\n",
    "affs_ds = daisy.open_ds(out_file, out_dataset)\n",
    "\n",
    "\n",
    "# Load predictions from other iteration\n",
    "iteration = 40000\n",
    "aff_file = 'affinities_{}_{:05}.zarr'.format(datasetsize, iteration)\n",
    "affs_ds_iter40000 = daisy.open_ds(aff_file, out_dataset)\n",
    "\n",
    "with viewer.txn() as s:\n",
    "    funlibng.add_layer(s, raw_ds, 'raw')\n",
    "    funlibng.add_layer(s, affs_ds, 'affs', shader='rgb')\n",
    "#     funlibng.add_layer(s, affs_ds_iter40000, 'affs_40000', shader='rgb')\n",
    "if len(ip_address) == 0:\n",
    "    print('you first have to set the ip address of your paperspace machine')\n",
    "else:\n",
    "    print(viewer.__str__().replace('localhost', ip_address))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Help & Troubleshooting\n",
    "- If your notebook is hanging, sometimes it helps to restart the kernel: For this go to `Kernel` and then `Restart & Clear Output`\n",
    "- If you get an error message: `compatible`, remove the container/file, that you are trying to write to. For instance, remove `affinities_big_50000.zarr` from the current directory if you have set this as `out_file`.\n",
    "- We observed, that after prediction is done, the notebook might die, so the output disappears, and it might seem to you, that nothing worked out, but this is not true. It usually dies, after the prediction has finished."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:segmentation]",
   "language": "python",
   "name": "conda-env-segmentation-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
