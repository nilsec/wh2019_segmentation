{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "As for training, we use `gunpowder` for predicting a larger area.\n",
    "## Exercise 1 : Start Inference\n",
    "1. Stop first training in `01_training.ipynb`\n",
    "2. Your task is to run the script twice, one time with `datasetsize` set to small, and one to big. With this we get two affinity predictions that we can play with in `03_agglomeration.ipynb`\n",
    "2. The default model that we use to make our prediction is a pretrained network. Change the `setup_dir`, such that it points to your training directory. By doing that, the network's weights are loaded from your model instead from our pretrained model. You also need to adapt `iteration` depending on what the last tensorflow checkpoint was, that are written out. For this go to your training directory, and search for the file with the highest number.\n",
    "\n",
    "Tip: It takes a while for the script to start a prediction. After aorund half a minute, you should be able to see following output. If you wait 2mins and nothing happens, restart the kernel and try again.\n",
    "```\n",
    "INFO:gunpowder.nodes.scan:processed chunk 1/288\n",
    "INFO:gunpowder.nodes.scan:processed chunk 2/288\n",
    "INFO:gunpowder.nodes.scan:processed chunk 3/288\n",
    "INFO:gunpowder.nodes.scan:processed chunk 4/288\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from gunpowder import *\n",
    "from gunpowder.tensorflow import *\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import daisy\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "def predict(iteration, in_file, out_file, setup_dir,\n",
    "            out_dataset, datasetsize='small'):\n",
    "\n",
    "    with open(os.path.join(setup_dir, 'train_net.json'), 'r') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    # voxels\n",
    "    voxel_size = Coordinate((40, 4, 4))\n",
    "    input_shape = Coordinate(config['input_shape']) # Network specific\n",
    "    output_shape = Coordinate(config['output_shape'])\n",
    "    context = (input_shape - output_shape) // 2\n",
    "    \n",
    "    # Initial ROI\n",
    "    # got nice offset visually from neuroglancer : 1507, 1678, 100\n",
    "    if datasetsize == 'big':\n",
    "        offset = (50, 1400, 1400) * np.array(voxel_size)  # avoid big glia and cell body in sample 0\n",
    "        chunkgrid = [2, 12, 12]\n",
    "\n",
    "    elif datasetsize == 'small':\n",
    "        offset = (90, 1600, 1400) * np.array(voxel_size)  \n",
    "        chunkgrid = [1, 5, 5]\n",
    "\n",
    "\n",
    "    roi = Roi(\n",
    "        offset=offset,\n",
    "        shape=output_shape * voxel_size * Coordinate(chunkgrid),\n",
    "    )\n",
    "\n",
    "\n",
    "    # nm\n",
    "    context_nm = context * voxel_size\n",
    "    read_roi = roi.copy()\n",
    "    read_roi = read_roi.grow(context_nm, context_nm)\n",
    "\n",
    "    input_size = input_shape * voxel_size\n",
    "    output_size = output_shape * voxel_size\n",
    "\n",
    "    output_roi = read_roi.grow(-context_nm, -context_nm)\n",
    "    print(\"Read ROI in nm is %s\" % read_roi)\n",
    "    print(\"Output ROI in nm is %s\" % output_roi)\n",
    "\n",
    "    print(\"Read ROI in voxel space is {}\".format(read_roi / voxel_size))\n",
    "    print(\"Output ROI in voxel space is {}\".format(output_roi / voxel_size))\n",
    "\n",
    "    raw = ArrayKey('RAW')\n",
    "    affs = ArrayKey('AFFS')\n",
    "\n",
    "    output_roi = daisy.Roi(\n",
    "        output_roi.get_begin(),\n",
    "        output_roi.get_shape()\n",
    "    )\n",
    "\n",
    "\n",
    "    # Caution: daisy.ROI and gunpowder.Roi have different behaviour, Prepare_ds only works with daisy.Roi, while\n",
    "    # gunpowder node eg. Crop Node only works with gunpowder.Roi\n",
    "    ds = daisy.prepare_ds(\n",
    "        out_file,\n",
    "        out_dataset,\n",
    "        output_roi,\n",
    "        voxel_size,\n",
    "        'float32',\n",
    "        # write_size=output_size,\n",
    "        write_roi=daisy.Roi((0, 0, 0), output_size),\n",
    "        num_channels=3,\n",
    "        # temporary fix until\n",
    "        # https://github.com/zarr-developers/numcodecs/pull/87 gets approved\n",
    "        # (we want gzip to be the default)\n",
    "        compressor={'id': 'gzip', 'level': 5}\n",
    "    )\n",
    "\n",
    "    chunk_request = BatchRequest()\n",
    "    chunk_request.add(raw, input_size)\n",
    "    chunk_request.add(affs, output_size)\n",
    "\n",
    "    pipeline = (\n",
    "            N5Source(\n",
    "                in_file,\n",
    "                datasets={\n",
    "                    raw: 'volumes/raw'\n",
    "                },\n",
    "            ) +\n",
    "            Pad(raw, size=None) +\n",
    "            Crop(raw, read_roi) +\n",
    "            Normalize(raw) +\n",
    "            IntensityScaleShift(raw, 2, -1) +\n",
    "            Predict(\n",
    "                os.path.join(setup_dir, 'train_net_checkpoint_%d' % iteration),\n",
    "                inputs={\n",
    "                    config['raw']: raw\n",
    "                },\n",
    "                outputs={\n",
    "                    config['affs']: affs\n",
    "                },\n",
    "                # TODO: change to predict graph\n",
    "                graph=os.path.join(setup_dir, 'train_net.meta')\n",
    "            ) +\n",
    "            IntensityScaleShift(raw, 0.5, 0.5) +  # Just for visualization.\n",
    "            ZarrWrite(\n",
    "                dataset_names={\n",
    "                    affs: out_dataset,\n",
    "                    raw: 'volumes/raw',\n",
    "                },\n",
    "                output_filename=out_file\n",
    "            ) +  # TODO: Would be nice to have a consistent file format (eg. only n5)\n",
    "            PrintProfilingStats(every=10) +\n",
    "            Scan(chunk_request)\n",
    "    )\n",
    "    start_time = time.time()\n",
    "    print(\"Starting prediction...\")\n",
    "    with build(pipeline):\n",
    "        pipeline.request_batch(BatchRequest())\n",
    "    print(\"Prediction finished in {:0.2f}\".format(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:daisy.datasets:write_roi is deprecated, please use write_size instead\n",
      "INFO:daisy.datasets:Creating new affinities_small_500000.zarr\n",
      "INFO:daisy.datasets:Creating new volumes/affs in affinities_small_500000.zarr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read ROI in nm is [2880:6240, 5976:7944, 5176:7144] (3360, 1968, 1968)\n",
      "Output ROI in nm is [3600:5520, 6400:7520, 5600:6720] (1920, 1120, 1120)\n",
      "Read ROI in voxel space is [72:156, 1494:1986, 1294:1786] (84, 492, 492)\n",
      "Output ROI in voxel space is [90:138, 1600:1880, 1400:1680] (48, 280, 280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gunpowder.tensorflow.local_server:Creating local tensorflow server\n",
      "INFO:gunpowder.tensorflow.local_server:Server running at b'grpc://localhost:36831'\n",
      "INFO:gunpowder.tensorflow.nodes.predict:Initializing tf session, connecting to b'grpc://localhost:36831'...\n",
      "INFO:gunpowder.tensorflow.nodes.predict:Reading graph from ../../jan/segmentation/snapshots/setup58_p/train_net.meta and weights from ../../jan/segmentation/snapshots/setup58_p/train_net_checkpoint_500000...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../../jan/segmentation/snapshots/setup58_p/train_net_checkpoint_500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../../jan/segmentation/snapshots/setup58_p/train_net_checkpoint_500000\n",
      "WARNING:gunpowder.nodes.hdf5like_source_base:WARNING: You didn't set 'interpolatable' for RAW (dataset volumes/raw). Based on the dtype uint8, it has been set to True. This might not be what you want.\n",
      "INFO:gunpowder.nodes.scan:scanning over 25 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gunpowder.nodes.scan:processed chunk 1/25\n",
      "INFO:gunpowder.nodes.scan:processed chunk 2/25\n",
      "INFO:gunpowder.nodes.scan:processed chunk 3/25\n",
      "INFO:gunpowder.nodes.scan:processed chunk 4/25\n",
      "INFO:gunpowder.nodes.scan:processed chunk 5/25\n",
      "INFO:gunpowder.nodes.scan:processed chunk 6/25\n",
      "INFO:gunpowder.nodes.scan:processed chunk 7/25\n",
      "INFO:gunpowder.nodes.scan:processed chunk 8/25\n",
      "INFO:gunpowder.nodes.scan:processed chunk 9/25\n",
      "INFO:gunpowder.nodes.print_profiling_stats:\n",
      "Profiling Stats\n",
      "===============\n",
      "\n",
      "NODE                METHOD    COUNTS    MIN       MAX       MEAN      MEDIAN    \n",
      "Crop                prepare   10        0.00      0.00      0.00      0.00      \n",
      "Crop                process   10        0.00      0.00      0.00      0.00      \n",
      "IntensityScaleShift prepare   20        0.00      0.00      0.00      0.00      \n",
      "IntensityScaleShift process   20        0.01      0.03      0.01      0.01      \n",
      "N5Source                      10        0.32      1.12      0.71      0.61      \n",
      "Normalize           prepare   10        0.00      0.00      0.00      0.00      \n",
      "Normalize           process   10        0.01      0.02      0.01      0.01      \n",
      "Pad                 prepare   10        0.00      0.00      0.00      0.00      \n",
      "Pad                 process   10        0.00      0.01      0.00      0.00      \n",
      "Predict             prepare   10        0.00      0.00      0.00      0.00      \n",
      "Predict             process   10        0.30      6.36      0.91      0.30      \n",
      "ZarrWrite           prepare   10        0.00      0.00      0.00      0.00      \n",
      "ZarrWrite           process   10        0.17      0.66      0.54      0.61      \n",
      "\n",
      "TOTAL\n",
      "upstream                      10        1.30      7.72      2.21      1.52      \n",
      "downstream                    9         0.00      0.01      0.00      0.00      \n",
      "\n",
      "\n",
      "INFO:gunpowder.nodes.scan:processed chunk 10/25\n",
      "INFO:gunpowder.nodes.scan:processed chunk 11/25\n",
      "INFO:gunpowder.nodes.scan:processed chunk 12/25\n",
      "INFO:gunpowder.nodes.scan:processed chunk 13/25\n",
      "INFO:gunpowder.nodes.scan:processed chunk 14/25\n",
      "INFO:gunpowder.nodes.scan:processed chunk 15/25\n",
      "INFO:gunpowder.nodes.scan:processed chunk 16/25\n",
      "INFO:gunpowder.nodes.scan:processed chunk 17/25\n",
      "INFO:gunpowder.nodes.scan:processed chunk 18/25\n",
      "INFO:gunpowder.nodes.scan:processed chunk 19/25\n",
      "INFO:gunpowder.nodes.print_profiling_stats:\n",
      "Profiling Stats\n",
      "===============\n",
      "\n",
      "NODE                METHOD    COUNTS    MIN       MAX       MEAN      MEDIAN    \n",
      "Crop                prepare   10        0.00      0.00      0.00      0.00      \n",
      "Crop                process   10        0.00      0.00      0.00      0.00      \n",
      "IntensityScaleShift prepare   20        0.00      0.00      0.00      0.00      \n",
      "IntensityScaleShift process   20        0.01      0.01      0.01      0.01      \n",
      "N5Source                      10        0.32      0.64      0.51      0.62      \n",
      "Normalize           prepare   10        0.00      0.00      0.00      0.00      \n",
      "Normalize           process   10        0.01      0.01      0.01      0.01      \n",
      "Pad                 prepare   10        0.00      0.00      0.00      0.00      \n",
      "Pad                 process   10        0.00      0.00      0.00      0.00      \n",
      "Predict             prepare   10        0.00      0.00      0.00      0.00      \n",
      "Predict             process   10        0.30      0.30      0.30      0.30      \n",
      "ZarrWrite           prepare   10        0.00      0.00      0.00      0.00      \n",
      "ZarrWrite           process   10        0.38      0.53      0.43      0.42      \n",
      "\n",
      "TOTAL\n",
      "upstream                      10        1.07      1.43      1.28      1.35      \n",
      "downstream                    10        0.00      0.01      0.00      0.00      \n",
      "\n",
      "\n",
      "INFO:gunpowder.nodes.scan:processed chunk 20/25\n",
      "INFO:gunpowder.nodes.scan:processed chunk 21/25\n",
      "INFO:gunpowder.nodes.scan:processed chunk 22/25\n",
      "INFO:gunpowder.nodes.scan:processed chunk 23/25\n",
      "INFO:gunpowder.nodes.scan:processed chunk 24/25\n",
      "INFO:gunpowder.nodes.scan:processed chunk 25/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction finished in 41.67\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger('gunpowder.nodes.hdf5like_write_base').setLevel(\n",
    "    logging.INFO)\n",
    "\n",
    "in_file = '../../jan/segmentation/data/sample_0.n5'  # This is our raw file\n",
    "setup_dir = '../../jan/segmentation/snapshots/setup58_p/' # This is the pretrained model. \n",
    "\n",
    "\n",
    "out_dataset = 'volumes/affs'\n",
    "datasetsize = 'small' # choose big or small\n",
    "iteration = 500000\n",
    "\n",
    "out_file = 'affinities_{}_{:05}.zarr'.format(datasetsize, iteration)\n",
    "\n",
    "predict(\n",
    "    iteration,\n",
    "    in_file,\n",
    "    out_file,\n",
    "    setup_dir,\n",
    "    out_dataset,\n",
    "    datasetsize=datasetsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 : Throughput calculations\n",
    "1. While you are waiting for the prediction to finish, you can calculate how many chunks needed to be predicted, if we were to process an entire fly brain\n",
    "  - the total size of the predicted dataset=big --> (3840, 2688, 2688) nm\n",
    "  - this translates to 288 chunks\n",
    "  - the dimension of the FAFB, a full adult fly brain dataset are: (282, 519, 991) microns\n",
    "\n",
    "2. After the prediction, you can make an estimate for how long it would take, if we only had a single gpu available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your solution here\n",
    "# -------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 : Check predicted affinities in neuroglancer\n",
    "1. Check out whether your prediction was successfull by exploring the result in `neuroglancer`\n",
    "2. You can predict datasets for multiple iterations, and compare them in neuroglancer\n",
    "3. If you are satisfied, you can move on to `03_agglomeration.ipynb`. If you have the feeling, your predictions are not correct (eg. only black), or you cant see any neuron border, consider to use the pretrained network instead of your own model. You need ok-affinity predictions for the next notebook.\n",
    "4. You might want to come back to this notebook, if you are interested in comparing eg. your model and our pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://184.105.98.58:40393/v/a574e98b2bfb02075db82f4c698df87b0cf260b7/\n"
     ]
    }
   ],
   "source": [
    "import neuroglancer\n",
    "import funlib.show.neuroglancer as funlibng\n",
    "\n",
    "ip_address = '184.105.98.58' # Put your ip adress here\n",
    "neuroglancer.set_server_bind_address('0.0.0.0')\n",
    "viewer = neuroglancer.Viewer()\n",
    "\n",
    "raw_ds = daisy.open_ds(in_file, 'volumes/raw')\n",
    "affs_ds = daisy.open_ds(out_file, out_dataset)\n",
    "\n",
    "with viewer.txn() as s:\n",
    "    funlibng.add_layer(s, raw_ds, 'raw')\n",
    "    funlibng.add_layer(s, affs_ds, 'affs', shader='rgb')\n",
    "if len(ip_address) == 0:\n",
    "    print('you first have to set the ip address of your paperspace machine')\n",
    "else:\n",
    "    print(viewer.__str__().replace('localhost', ip_address))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Help & Troubleshooting\n",
    "- If your notebook is hanging, sometimes it helps to restart the kernel: For this go to `Kernel` and then `Restart & Clear Output`\n",
    "- If you get an error message: `compatible`, remove the container/file, that you are trying to write to. For instance, remove `affinities_big_50000.zarr` from the current directory if you have set this as `out_file`.\n",
    "- We observed, that after prediction is done, the notebook might die, so the output disappears, and it might seem to you, that nothing worked out, but this is not true. It usually dies, after the prediction has finished."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:segmentation]",
   "language": "python",
   "name": "conda-env-segmentation-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
