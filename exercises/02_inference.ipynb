{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "As for training, we use `gunpowder` for predicting a larger area.\n",
    "## Exercise 1 : Start Inference\n",
    "1. Your task is to run the script twice, one time with `datasetsize` set to small, and one to big. With this we get two affinity predictions that we can play with in `03_agglomeration.ipynb`\n",
    "2. The default model that we use to make our prediction is a pretrained network. Change the `setup_dir`, such that it points to your training directory. By doing that, the network's weights are loaded from your model instead from our pretrained model. You also need to adapt `iteration` depending on what the last tensorflow checkpoint was, that are written out. For this go to your training directory, and search for this file: ``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from gunpowder import *\n",
    "from gunpowder.tensorflow import *\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import daisy\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "def predict(iteration, in_file, out_file, setup_dir,\n",
    "            out_dataset, datasetsize='small'):\n",
    "\n",
    "    with open(os.path.join(setup_dir, 'train_net.json'), 'r') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    # voxels\n",
    "    voxel_size = Coordinate((40, 4, 4))\n",
    "    input_shape = Coordinate(config['input_shape']) # Network specific\n",
    "    output_shape = Coordinate(config['output_shape'])\n",
    "    context = (input_shape - output_shape) // 2\n",
    "    \n",
    "    # Initial ROI\n",
    "    # got nice offset visually from neuroglancer : 1507, 1678, 100\n",
    "    if datasetsize == 'big':\n",
    "        offset = (50, 1400, 1400) * np.array(voxel_size)  # avoid big glia and cell body in sample 0\n",
    "        chunkgrid = [2, 12, 12]\n",
    "\n",
    "    elif datasetsize == 'small':\n",
    "        offset = (90, 1600, 1400) * np.array(voxel_size)  \n",
    "        chunkgrid = [1, 5, 5]\n",
    "\n",
    "\n",
    "    roi = Roi(\n",
    "        offset=offset,\n",
    "        shape=output_shape * voxel_size * Coordinate(chunkgrid),\n",
    "    )\n",
    "\n",
    "\n",
    "    # nm\n",
    "    context_nm = context * voxel_size\n",
    "    read_roi = roi.copy()\n",
    "    read_roi = read_roi.grow(context_nm, context_nm)\n",
    "\n",
    "    input_size = input_shape * voxel_size\n",
    "    output_size = output_shape * voxel_size\n",
    "\n",
    "    output_roi = read_roi.grow(-context_nm, -context_nm)\n",
    "    print(\"Read ROI in nm is %s\" % read_roi)\n",
    "    print(\"Output ROI in nm is %s\" % output_roi)\n",
    "\n",
    "    print(\"Read ROI in voxel space is {}\".format(read_roi / voxel_size))\n",
    "    print(\"Output ROI in voxel space is {}\".format(output_roi / voxel_size))\n",
    "\n",
    "    raw = ArrayKey('RAW')\n",
    "    affs = ArrayKey('AFFS')\n",
    "\n",
    "    output_roi = daisy.Roi(\n",
    "        output_roi.get_begin(),\n",
    "        output_roi.get_shape()\n",
    "    )\n",
    "\n",
    "\n",
    "    # Caution: daisy.ROI and gunpowder.Roi have different behaviour, Prepare_ds only works with daisy.Roi, while\n",
    "    # gunpowder node eg. Crop Node only works with gunpowder.Roi\n",
    "    ds = daisy.prepare_ds(\n",
    "        out_file,\n",
    "        out_dataset,\n",
    "        output_roi,\n",
    "        voxel_size,\n",
    "        'float32',\n",
    "        # write_size=output_size,\n",
    "        write_roi=daisy.Roi((0, 0, 0), output_size),\n",
    "        num_channels=3,\n",
    "        # temporary fix until\n",
    "        # https://github.com/zarr-developers/numcodecs/pull/87 gets approved\n",
    "        # (we want gzip to be the default)\n",
    "        compressor={'id': 'gzip', 'level': 5}\n",
    "    )\n",
    "\n",
    "    chunk_request = BatchRequest()\n",
    "    chunk_request.add(raw, input_size)\n",
    "    chunk_request.add(affs, output_size)\n",
    "\n",
    "    pipeline = (\n",
    "            N5Source(\n",
    "                in_file,\n",
    "                datasets={\n",
    "                    raw: 'volumes/raw'\n",
    "                },\n",
    "            ) +\n",
    "            Pad(raw, size=None) +\n",
    "            Crop(raw, read_roi) +\n",
    "            Normalize(raw) +\n",
    "            IntensityScaleShift(raw, 2, -1) +\n",
    "            Predict(\n",
    "                os.path.join(setup_dir, 'train_net_checkpoint_%d' % iteration),\n",
    "                inputs={\n",
    "                    config['raw']: raw\n",
    "                },\n",
    "                outputs={\n",
    "                    config['affs']: affs\n",
    "                },\n",
    "                # TODO: change to predict graph\n",
    "                graph=os.path.join(setup_dir, 'train_net.meta')\n",
    "            ) +\n",
    "            IntensityScaleShift(raw, 0.5, 0.5) +  # Just for visualization.\n",
    "            ZarrWrite(\n",
    "                dataset_names={\n",
    "                    affs: out_dataset,\n",
    "                    raw: 'volumes/raw',\n",
    "                },\n",
    "                output_filename=out_file\n",
    "            )\n",
    "            PrintProfilingStats(every=10) +\n",
    "            Scan(chunk_request)\n",
    "    )\n",
    "    start_time = time.time()\n",
    "    print(\"Starting prediction...\")\n",
    "    with build(pipeline):\n",
    "        pipeline.request_batch(BatchRequest())\n",
    "    print(\"Prediction finished in {:0.2f}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger('gunpowder.nodes.hdf5like_write_base').setLevel(\n",
    "    logging.DEBUG)\n",
    "\n",
    "in_file = '../jan/segmentation/data/sample_0.n5'  # This is our raw file\n",
    "setup_dir = '../../jan/segmentation/snapshots/setup58_p/' # This is the pretrained model. Change\n",
    "# this to the train directory.\n",
    "\n",
    "out_dataset = 'volumes/affs'\n",
    "datasetsize = 'big' # choose big or small\n",
    "iteration = 500000\n",
    "\n",
    "out_file = 'affinities_{}_{:05}.zarr'.format(datasetsize, iteration)\n",
    "\n",
    "predict(\n",
    "    iteration,\n",
    "    in_file,\n",
    "    out_file,\n",
    "    setup_dir,\n",
    "    out_dataset,\n",
    "    datasetsize=datasetsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 : Throughput calculations\n",
    "1. While you are waiting for the prediction to finish, you can calculate how many chunks needed to be predicted, if we were to process an entire fly brain\n",
    "  - the dimension of the FAFB, a full adult fly brain dataset are: []\n",
    "  - a chunk size is: [, , ] voxels, which translates to [] microns\n",
    "2. After the prediction, you can make an estimate for how long it would take, if we only had a single gpu available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 : Check predicted affinities in neuroglancer\n",
    "1. Check out whether your prediction was successfull by exploring the result in `neuroglancer`\n",
    "2. If you are satisfied, you can move on to `03_agglomeration.ipynb`. If you have the feeling, your predictions are not correct (eg. only black), or you cant see any neuron border, consider to use the pretrained network instead of your own model. You need ok-affinity predictions for the next notebook.\n",
    "3. You might want to come back to this notebook, if you are interested in comparing eg. your model and our pretrained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Help & Troubleshooting\n",
    "- If your notebook is hanging, sometimes it helps to restart the kernel: For this go to `Kernel` and then `Restart & Clear Output`\n",
    "- If you get an error message: `compatible`, remove the container/file, that you are trying to write to. For instance, remove `affinities_big_50000.zarr` from the current directory if you have set this as `out_file`.\n",
    "- We observed, that after prediction is done, the notebook might die, so the output disappears, and it might seem to you, that nothing worked out, but this is not true. It usually dies, after the prediction has finished."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
