{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "As for training, we use `gunpowder` for predicting a larger area.\n",
    " \n",
    "(explain chunk processing)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "Your task is to run the script twice, one time with datasetsize set to small, and one to big. With this we get two affinity prediction that we can play with in `03_agglomeration.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "1) While you are waiting for the prediction to finish, you can calculate how many chu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from gunpowder import *\n",
    "from gunpowder.tensorflow import *\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import daisy\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "def predict(iteration, in_file, out_file, setup_dir,\n",
    "            out_dataset, datasetsize='small'):\n",
    "\n",
    "    with open(os.path.join(setup_dir, 'train_net.json'), 'r') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    # voxels\n",
    "    voxel_size = Coordinate((40, 4, 4))\n",
    "    input_shape = Coordinate(config['input_shape'])\n",
    "    output_shape = Coordinate(config['output_shape'])\n",
    "    context = (input_shape - output_shape) // 2\n",
    "    if datasetsize == 'big':\n",
    "        offset = (50, 1400, 1400) * np.array(voxel_size)  # avoid big glia and cell body in sample 0\n",
    "        chunkgrid = [2, 12, 12]\n",
    "\n",
    "    elif datasetsize == 'small':\n",
    "        offset = (90, 1600, 1400) * np.array(voxel_size)  # avoid big glia\n",
    "        chunkgrid = [1, 5, 5]\n",
    "\n",
    "\n",
    "\n",
    "    # Initial ROI\n",
    "    # got nice offset visually from neuroglancer : 1507, 1678, 100\n",
    "    roi = Roi(\n",
    "        offset=offset,\n",
    "        shape=output_shape * voxel_size * Coordinate(chunkgrid),\n",
    "    )\n",
    "\n",
    "\n",
    "    # nm\n",
    "    context_nm = context * voxel_size\n",
    "    read_roi = roi.copy()\n",
    "    read_roi = read_roi.grow(context_nm, context_nm)\n",
    "\n",
    "\n",
    "    # read_roi = read_roi.snap_to_grid(input_shape * voxel_size)\n",
    "    input_size = input_shape * voxel_size\n",
    "    output_size = output_shape * voxel_size\n",
    "\n",
    "    output_roi = read_roi.grow(-context_nm, -context_nm)\n",
    "    print(\"Read ROI in nm is %s\" % read_roi)\n",
    "    print(\"Output ROI in nm is %s\" % output_roi)\n",
    "\n",
    "    print(\"Read ROI in voxel space is {}\".format(read_roi / voxel_size))\n",
    "    print(\"Output ROI in voxel space is {}\".format(output_roi / voxel_size))\n",
    "\n",
    "    raw = ArrayKey('RAW')\n",
    "    affs = ArrayKey('AFFS')\n",
    "\n",
    "    output_roi = daisy.Roi(\n",
    "        output_roi.get_begin(),\n",
    "        output_roi.get_shape()\n",
    "    )\n",
    "\n",
    "    # TODO: Introduces daisy dependency, does that work without ?\n",
    "    # Also, daisy.ROI and gunpowder.Roi have different behaviour, important\n",
    "    # source of confusion. Prepare_ds only works with daisy.Roi, while\n",
    "    # gunpowder node eg. Crop only works with gunpowder.Roi\n",
    "    ds = daisy.prepare_ds(\n",
    "        out_file,\n",
    "        out_dataset,\n",
    "        output_roi,\n",
    "        voxel_size,\n",
    "        'float32',\n",
    "        # write_size=output_size,\n",
    "        write_roi=daisy.Roi((0, 0, 0), output_size),\n",
    "        num_channels=3,\n",
    "        # temporary fix until\n",
    "        # https://github.com/zarr-developers/numcodecs/pull/87 gets approved\n",
    "        # (we want gzip to be the default)\n",
    "        compressor={'id': 'gzip', 'level': 5}\n",
    "    )\n",
    "\n",
    "    chunk_request = BatchRequest()\n",
    "    chunk_request.add(raw, input_size)\n",
    "    chunk_request.add(affs, output_size)\n",
    "\n",
    "    pipeline = (\n",
    "            N5Source(\n",
    "                in_file,\n",
    "                datasets={\n",
    "                    raw: 'volumes/raw'\n",
    "                },\n",
    "            ) +\n",
    "            Pad(raw, size=None) +\n",
    "            Crop(raw, read_roi) +\n",
    "            Normalize(raw) +\n",
    "            IntensityScaleShift(raw, 2, -1) +\n",
    "            Predict(\n",
    "                os.path.join(setup_dir, 'train_net_checkpoint_%d' % iteration),\n",
    "                inputs={\n",
    "                    config['raw']: raw\n",
    "                },\n",
    "                outputs={\n",
    "                    config['affs']: affs\n",
    "                },\n",
    "                # TODO: change to predict graph\n",
    "                graph=os.path.join(setup_dir, 'train_net.meta')\n",
    "            ) +\n",
    "            IntensityScaleShift(raw, 0.5, 0.5) +  # Just for visualization.\n",
    "            ZarrWrite(\n",
    "                dataset_names={\n",
    "                    affs: out_dataset,\n",
    "                    raw: 'volumes/raw',\n",
    "                },\n",
    "                output_filename=out_file\n",
    "            ) +  # TODO: Would be nice to have a consistent file format (eg. only n5)\n",
    "            PrintProfilingStats(every=10) +\n",
    "            Scan(chunk_request)\n",
    "    )\n",
    "    start_time = time.time()\n",
    "    print(\"Starting prediction...\")\n",
    "#     with build(pipeline):\n",
    "#         pipeline.request_batch(BatchRequest())\n",
    "    print(\"Prediction finished in {:0.2f}\".format(time.time() - start_time))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger('gunpowder.nodes.hdf5like_write_base').setLevel(\n",
    "    logging.DEBUG)\n",
    "\n",
    "in_file = '../data/sample_0.n5'  # This is our raw file\n",
    "setup_dir = '../data/setup58_p/' # This is the pretrained model. Change\n",
    "# this to the train directory.\n",
    "\n",
    "out_dataset = 'volumes/affs'\n",
    "datasetsize = 'big' # choose big or small\n",
    "iteration = 500000\n",
    "\n",
    "out_file = 'affinities_{}_{:05}.zarr'.format(datasetsize, iteration)\n",
    "\n",
    "predict(\n",
    "    iteration,\n",
    "    in_file,\n",
    "    out_file,\n",
    "    setup_dir,\n",
    "    out_dataset,\n",
    "    datasetsize=datasetsize)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
