{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import daisy\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import ndimage\n",
    "import funlib.segment\n",
    "import malis\n",
    "import matplotlib.gridspec as gridspec\n",
    "import waterz\n",
    "import logging\n",
    "from importlib import reload\n",
    "import utils\n",
    "import funlib.show.neuroglancer as funlibng\n",
    "import neuroglancer\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the inference results\n",
    "1) make sure you predicted affinities with inference.ipynb\n",
    "2) You have two different datasetsizes written out,examples are nicer with datasetsize=big, but if the functions are too slow for you, consider to switch to datasetsize=small\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "nothing found at path ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-de781a8c5a63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0maffs_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdaisy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_ds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maff_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'volumes/affs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mraw_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdaisy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_ds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maff_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'volumes/raw'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mgtds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdaisy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_ds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'volumes/labels/neuron_ids'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mroi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maffs_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgtds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# The Region of Interest that we will look at in this notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/wh2019_segmentation/src/daisy/daisy/datasets.py\u001b[0m in \u001b[0;36mopen_ds\u001b[0;34m(filename, ds_name, mode)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"opening N5 dataset %s in %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzarr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mds_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mvoxel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_voxel_size_offset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'F'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/wh2019_segmentation/src/zarr/zarr/convenience.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(store, mode, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0merr_path_not_found\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/wh2019_segmentation/src/zarr/zarr/errors.py\u001b[0m in \u001b[0;36merr_path_not_found\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0merr_path_not_found\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nothing found at path %r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: nothing found at path ''"
     ]
    }
   ],
   "source": [
    "# Load the data (make sure you ran inference.ipynb before).\n",
    "# By default, you wrote out the big ROI at iteration 500000. However, if you changed iteration \n",
    "# or datasetsize, change this here\n",
    "datasetsize = 'big' # change accordingly\n",
    "iteration = 500000 # Change to check out other training iterations\n",
    "aff_file = 'affinities_{}_{:05}.zarr'.format(datasetsize, iteration)\n",
    "\n",
    "# --------- Do Not Change --------\n",
    "gt_file = '../data/sample_0.n5'\n",
    "affs_ds = daisy.open_ds(aff_file, 'volumes/affs')\n",
    "raw_ds = daisy.open_ds(aff_file, 'volumes/raw')\n",
    "gtds = daisy.open_ds(gt_file, 'volumes/labels/neuron_ids')\n",
    "\n",
    "roi = affs_ds.roi.intersect(gtds.roi) # The Region of Interest that we will look at in this notebook\n",
    "\n",
    "gt = gtds.to_ndarray(roi)\n",
    "raw = raw_ds.to_ndarray(roi)\n",
    "affs = affs_ds.to_ndarray(roi)\n",
    "\n",
    "z_section = 9 # For 2d Visualization, always pick the same z_section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affinities are in z,y,x direction, here average over x and y vor visualization.\n",
    "mean_affs = 0.5*(affs[1] + affs[2]) \n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "gs = gridspec.GridSpec(1, 3)\n",
    "\n",
    "ax = plt.subplot(gs[0,0])\n",
    "plt.imshow(np.squeeze(mean_affs[z_section, :, :]), cmap='gray')\n",
    "ax.axis('off')  # clear x- and y-axes\n",
    "plt.title('Predicted Affinities')\n",
    "\n",
    "ax = plt.subplot(gs[0,1])\n",
    "ax.imshow(raw[z_section, :], cmap='gray')\n",
    "ax.axis('off')  # clear x- and y-axes\n",
    "plt.title('Raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain a segmentation by thresholding\n",
    "Given a perfect boundary prediction, obtaining a segmentation would be trivial. It would be enough to create a binary mask by thresholding and then finding connected component. As we will see, minor errors in our boundary predictions can lead to catastropical errors in our segmentation\n",
    "### 2D example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connected_component_sectionwise(affs, threshold):\n",
    "    mean_affs = 0.5*(affs[1] + affs[2])\n",
    "    depth = mean_affs.shape[0]\n",
    "    cc = np.zeros(mean_affs.shape, dtype=np.uint64)\n",
    "    boundary = np.zeros(mean_affs.shape, dtype=np.uint8)\n",
    "    for z in range(depth):\n",
    "        boundary_mask = mean_affs[z] > 0.5\n",
    "\n",
    "        boundarysingle = mean_affs[z, :] > threshold\n",
    "        ccsingle, num_labels = ndimage.label(boundarysingle)\n",
    "        cc[z] = ccsingle\n",
    "        boundary[z] = boundarysingle\n",
    "    return cc, boundary\n",
    "\n",
    "threshold = 0.3\n",
    "cc, boundary = connected_component_sectionwise(affs, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "gs = gridspec.GridSpec(1, 3)\n",
    "\n",
    "ax = plt.subplot(gs[0,0])\n",
    "ax.imshow(boundary[z_section, :], cmap='gray')\n",
    "ax.axis('off')  # clear x- and y-axes\n",
    "plt.title('Binary')\n",
    "\n",
    "ax = plt.subplot(gs[0,1])\n",
    "ax.imshow(cc[z_section, :], cmap='viridis')\n",
    "ax.axis('off')  # clear x- and y-axes\n",
    "plt.title('ConnectedComponents')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the snapshots, to get a good impression, it is better to explore the data in 3D with neuroglancer. Also, two neighboring neurons might by accident have the same color, though their ID is different. Neuroglancer allows you to \n",
    "explore this in more detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuroglancer.set_server_bind_address('0.0.0.0')\n",
    "viewer = neuroglancer.Viewer()\n",
    "with viewer.txn() as s:\n",
    "    funlibng.add_layer(s, raw_ds, 'raw')\n",
    "    funlibng.add_layer(s, affs_ds, 'affs', shader='rgb')\n",
    "    funlibng.add_layer(s, daisy.Array(cc, roi, affs_ds.voxel_size), 'connected components')\n",
    "    funlibng.add_layer(s, daisy.Array(boundary, roi, affs_ds.voxel_size), 'Binary')\n",
    "print(viewer.__str__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "Try out connected components for different thresholds and explore the effect in neuroglancer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you should type your solution\n",
    "# ......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D example\n",
    "The 2D example already shows, that some neurons accidently gets merged, when there is a small hole in the membrane. This gets worse, if you do the same procedure in 3D.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhood = [[-1, 0, 0], [0, -1, 0], [0, 0, -1]]\n",
    "cc_3d, __ = malis.connected_components_affgraph(np.array(affs > threshold, dtype=np.int32),np.array(nhood))\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "\n",
    "im = ax.imshow(cc_3d[z_section, :], cmap='viridis')\n",
    "ax.axis('off')  # clear x- and y-axes\n",
    "plt.title('Connected Component from Affinity Graph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuroglancer.set_server_bind_address('0.0.0.0')\n",
    "viewer = neuroglancer.Viewer()\n",
    "with viewer.txn() as s:\n",
    "    funlibng.add_layer(s, raw_ds, 'raw')\n",
    "    funlibng.add_layer(s, affs_ds, 'affs', shader='rgb')\n",
    "    funlibng.add_layer(s, daisy.Array(cc_3d, roi, affs_ds.voxel_size), 'connected components')\n",
    "print(viewer.__str__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Watershed and Fragment Agglomeration for a more robust segmentation\n",
    "## Creation of Fragments\n",
    "We use the watershed algorithm to create an oversegmentation. The Watershed is more robust towards small errors in the hole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = utils.apply_watershed(affs, fragments_in_xy=True)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "gs = gridspec.GridSpec(1, 3)\n",
    "\n",
    "ax = plt.subplot(gs[0,0])\n",
    "plt.imshow(fragments[z_section, :], cmap='viridis')\n",
    "ax.axis('off')  # clear x- and y-axes\n",
    "plt.title('Initial Fragments')\n",
    "\n",
    "ax = plt.subplot(gs[0,1])\n",
    "ax.imshow(raw[z_section, :], cmap='gray')\n",
    "ax.axis('off')  # clear x- and y-axes\n",
    "plt.title('Raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomeration\n",
    "The watershed is good to prevent too much merging. However, we can clearly see, that now the neurons are broken into too many pieces. In this part, we will see, how we can agglomerate those little pieces / fragments into larger ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'affs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9e9868269d0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msegments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magglomerate_fragments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maffs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfragments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'affs' is not defined"
     ]
    }
   ],
   "source": [
    "threshold = 0.9\n",
    "segments = utils.agglomerate_fragments(affs, fragments, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "gs = gridspec.GridSpec(1, 3)\n",
    "\n",
    "ax = plt.subplot(gs[0,0])\n",
    "plt.imshow(np.array(segments[z_section, :], dtype=np.float32), cmap='viridis')\n",
    "ax.axis('off')  # clear x- and y-axes\n",
    "plt.title('Segments')\n",
    "\n",
    "ax = plt.subplot(gs[0,1])\n",
    "ax.imshow(raw[z_section, :], cmap='gray')\n",
    "ax.axis('off')  # clear x- and y-axes\n",
    "plt.title('Raw')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: How does the segmentation change when we change the agglomeration threshold ?\n",
    "1) Try to find a threshold, that produces meaningful results (this means, ideally, no two neurons should be merged together, and not a single neuron should be split into two segments\n",
    "2) We also add our ground truth data here, such that you see, what we are aiming for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you should type your solution\n",
    "# 1) apply agglomeration functions for different thresholds\n",
    "# 2) see the result in neuroglancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuroglancer.set_server_bind_address('0.0.0.0')\n",
    "viewer = neuroglancer.Viewer()\n",
    "with viewer.txn() as s:\n",
    "    funlibng.add_layer(s, raw_ds, 'raw')\n",
    "    funlibng.add_layer(s, affs_ds, 'affs', shader='rgb')\n",
    "    funlibng.add_layer(s, daisy.Array(fragments, roi, affs_ds.voxel_size), 'Fragments')\n",
    "    funlibng.add_layer(s, daisy.Array(segments, roi, affs_ds.voxel_size), 'Segmentation')\n",
    "    funlibng.add_layer(s, daisy.Array(gt, roi, affs_ds.voxel_size), 'Ground Truth')\n",
    "    # Add here more segmentation with different thresholds\n",
    "    \n",
    "    # funlibng.add_layer(s, daisy.Array(segment_thr05, roi, affs_ds.voxel_size), 'Segmentation')\n",
    "\n",
    "    print(viewer.__str__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation (Extra Exercises)\n",
    "So far, we only assessed qualitatively our results. Now, we will have a look how we can quantitavely check the performance our of our models. For this, we use our ground truth data to compare it to our predicted segmentation,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt, __ = funlib.segment.arrays.relabel(gt, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 22})\n",
    "plt.figure(figsize=(20, 20))\n",
    "gs = gridspec.GridSpec(1, 3)\n",
    "\n",
    "ax = plt.subplot(gs[0,0])\n",
    "plt.imshow(np.squeeze(affs[0, z_section, :]), cmap='gray')\n",
    "ax.set_title('Affinities')\n",
    "ax.axis('off') \n",
    "ax = plt.subplot(gs[0,1])\n",
    "ax.imshow(np.squeeze(gt[z_section, :]), cmap='viridis')\n",
    "ax.set_title('Ground Truth')\n",
    "ax.axis('off') \n",
    "ax = plt.subplot(gs[0,2])\n",
    "ax.imshow(segments[z_section, :], cmap='viridis')\n",
    "ax.set_title('Predicted Segmentation')\n",
    "ax.axis('off') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 2 measurements for evaluation: `Rand Index` and `VOI`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = waterz.evaluate(gt, segments)\n",
    "print(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: How does VOI_split and VOI_merge change with changing agglomeration thresholds ?\n",
    "In this exercise you should\n",
    "\n",
    "  1. create multiple segmentations for different thresholds\n",
    "  2. Evalute each segmentation against the ground truth segmentation (with the function above: `waterz.evaluate`)\n",
    "  3. Make a a) plot with the threshold plotted against VOI_split and b) a plot with the threshold plotted against VOI_merge\n",
    "  4. Check out the [CREMI benchmark](www.cremi.org) and see what state of the art results are and put your numbers into context\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you should type your solution\n",
    "# ......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5 (extra, extra, extra :)) : Does the same for different checkpoints\n",
    "eg. how do those numbers evolve during training, eg. at 30k, 60k ?\n",
    "In this exercise you should\n",
    "\n",
    "  1. create multiple segmentations for different iterations\n",
    "  2. Evalute each segmentation against the ground truth segmentation\n",
    "  3. Make a a) plot with the iteration plotted against VOI_split and b) a plot with the iteration plotted against VOI_merge\n",
    "  4. Check out the [CREMI benchmark](www.cremi.org) and see what state of the art results are"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
