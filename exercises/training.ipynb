{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to EM data, neuron segmentation and gunpowder\n",
    "### A simple gunpowder example pipeline for loading and manipulating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from gunpowder import *\n",
    "from gunpowder.tensorflow import *\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "# A simple gunpowder pipeline for loading and manipulating data:\n",
    "data_dir = '../../woodshole/segmentation/data'\n",
    "\n",
    "samples = [\n",
    "    'sample_A',\n",
    "    'sample_B',\n",
    "    'sample_C'\n",
    "]\n",
    "\n",
    "\n",
    "# Define gunpowder variables of interest:\n",
    "raw = ArrayKey('RAW') # Raw EM data\n",
    "labels = ArrayKey('GT_LABELS') # ground truth neuron segmentation \n",
    "affinities = ArrayKey('GT_AFFINITIES') # affinities\n",
    "\n",
    "# Voxel size is the physical size of one voxel (=3D pixel) in nm.\n",
    "voxel_size = Coordinate((40, 4, 4))\n",
    "batch_size = Coordinate((30,1000,1000)) * voxel_size\n",
    "\n",
    "# Request all the data you need for training:\n",
    "request = BatchRequest()\n",
    "request.add(raw, batch_size)\n",
    "request.add(labels, batch_size)\n",
    "request.add(affinities, batch_size)\n",
    "\n",
    "# Request a snapshot s.t. you are able to visualize the data in your pipeline.\n",
    "snapshot_request = BatchRequest({\n",
    "    raw: request[raw],\n",
    "    labels: request[labels],\n",
    "    affinities: request[affinities]\n",
    "})\n",
    "\n",
    "# Note that the data only provides raw and neuron_ids which is the neuron segmentation.\n",
    "# However, we need affinities which we can generate from neuron_ids by using gunpowder (see below):\n",
    "data_sources = tuple(\n",
    "    N5Source(\n",
    "        os.path.join(data_dir, sample + '.n5'),\n",
    "        datasets = {\n",
    "            raw: 'volumes/raw',\n",
    "            labels: 'volumes/labels/neuron_ids',\n",
    "        },\n",
    "        array_specs = {\n",
    "            raw: ArraySpec(interpolatable=True),\n",
    "            labels: ArraySpec(interpolatable=False)\n",
    "        }\n",
    "    ) +\n",
    "    Normalize(raw) + \n",
    "    Pad(labels, None) +\n",
    "    RandomLocation()\n",
    "    for sample in samples\n",
    "    )\n",
    "\n",
    "\n",
    "# Define a neighborhood for affinities:\n",
    "neighborhood = [[-1, 0, 0], [0, -1, 0], [0, 0, -1]]\n",
    "\n",
    "pipeline = (\n",
    "    data_sources +\n",
    "    RandomProvider() +\n",
    "    AddAffinities(neighborhood,\n",
    "                 labels=labels,\n",
    "                 affinities=affinities) +\n",
    "    Snapshot({raw: 'volumes/raw',\n",
    "              labels: 'volumes/labels/neuron_ids',\n",
    "              affinities: 'volumes/affinities'}))\n",
    "    \n",
    "    \n",
    "with build(pipeline) as b:\n",
    "    b.request_batch(request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize data using neuroglancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using gunpowder with tensorflow for training a 3D-UNet for affinity prediction\n",
    "## mknet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating U-Net layer 0\n",
      "f_in: (1, 1, 84, 268, 268)\n",
      "number of variables added: 4236, new total: 4236\n",
      "    Creating U-Net layer 1\n",
      "    f_in: (1, 12, 80, 88, 88)\n",
      "    number of variables added: 116760, new total: 120996\n",
      "        Creating U-Net layer 2\n",
      "        f_in: (1, 60, 76, 28, 28)\n",
      "        number of variables added: 2916600, new total: 3037596\n",
      "            Creating U-Net layer 3\n",
      "            f_in: (1, 300, 24, 8, 8)\n",
      "            bottom layer\n",
      "            f_out: (1, 1500, 20, 4, 4)\n",
      "            number of variables added: 72903000, new total: 75940596\n",
      "        g_out: (1, 1500, 20, 4, 4)\n",
      "        g_out_upsampled: (1, 300, 60, 12, 12)\n",
      "        f_left_cropped: (1, 300, 60, 12, 12)\n",
      "        f_right: (1, 600, 60, 12, 12)\n",
      "        f_out: (1, 300, 56, 8, 8)\n",
      "        number of variables added: 19440900, new total: 95381496\n",
      "    g_out: (1, 300, 56, 8, 8)\n",
      "    g_out_upsampled: (1, 60, 56, 24, 24)\n",
      "    f_left_cropped: (1, 60, 56, 24, 24)\n",
      "    f_right: (1, 120, 56, 24, 24)\n",
      "    f_out: (1, 60, 52, 20, 20)\n",
      "    number of variables added: 453780, new total: 95835276\n",
      "g_out: (1, 60, 52, 20, 20)\n",
      "g_out_upsampled: (1, 12, 52, 60, 60)\n",
      "f_left_cropped: (1, 12, 52, 60, 60)\n",
      "f_right: (1, 24, 52, 60, 60)\n",
      "f_out: (1, 12, 48, 56, 56)\n",
      "number of variables added: 18180, new total: 95853456\n",
      "WARNING:tensorflow:From /groups/funke/home/ecksteinn/miniconda2/envs/segmentation/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:667: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "input shape : (84, 268, 268)\n",
      "output shape: [48, 56, 56]\n",
      "Creating U-Net layer 0\n",
      "f_in: (1, 1, 114, 646, 646)\n",
      "number of variables added: 4236, new total: 4236\n",
      "    Creating U-Net layer 1\n",
      "    f_in: (1, 12, 110, 214, 214)\n",
      "    number of variables added: 116760, new total: 120996\n",
      "        Creating U-Net layer 2\n",
      "        f_in: (1, 60, 106, 70, 70)\n",
      "        number of variables added: 2916600, new total: 3037596\n",
      "            Creating U-Net layer 3\n",
      "            f_in: (1, 300, 34, 22, 22)\n",
      "            bottom layer\n",
      "            f_out: (1, 1500, 30, 18, 18)\n",
      "            number of variables added: 72903000, new total: 75940596\n",
      "        g_out: (1, 1500, 30, 18, 18)\n",
      "        g_out_upsampled: (1, 300, 90, 54, 54)\n",
      "        f_left_cropped: (1, 300, 90, 54, 54)\n",
      "        f_right: (1, 600, 90, 54, 54)\n",
      "        f_out: (1, 300, 86, 50, 50)\n",
      "        number of variables added: 19440900, new total: 95381496\n",
      "    g_out: (1, 300, 86, 50, 50)\n",
      "    g_out_upsampled: (1, 60, 86, 150, 150)\n",
      "    f_left_cropped: (1, 60, 86, 150, 150)\n",
      "    f_right: (1, 120, 86, 150, 150)\n",
      "    f_out: (1, 60, 82, 146, 146)\n",
      "    number of variables added: 453780, new total: 95835276\n",
      "g_out: (1, 60, 82, 146, 146)\n",
      "g_out_upsampled: (1, 12, 82, 438, 438)\n",
      "f_left_cropped: (1, 12, 82, 438, 438)\n",
      "f_right: (1, 24, 82, 438, 438)\n",
      "f_out: (1, 12, 78, 434, 434)\n",
      "number of variables added: 18180, new total: 95853456\n",
      "input shape : (114, 646, 646)\n",
      "output shape: [78, 434, 434]\n"
     ]
    }
   ],
   "source": [
    "from funlib.learn.tensorflow import models\n",
    "import malis\n",
    "import tensorflow as tf\n",
    "import json\n",
    "\n",
    "def create_network(input_shape, name):\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    with tf.variable_scope('setup0'):\n",
    "\n",
    "        raw = tf.placeholder(tf.float32, shape=input_shape)\n",
    "        raw_batched = tf.reshape(raw, (1, 1) + input_shape)\n",
    "        \n",
    "        \"\"\"\n",
    "        Define your model architecture here:\n",
    "        \n",
    "        1. Set the main model parameters for the UNet:\n",
    "        \n",
    "        unet, _, _ = models.unet(...)\n",
    "        \n",
    "        2. Generate the desired number of output features (3 for affinities in z,y,x direction)\n",
    "           by using a convolution with kernel size 1:\n",
    "           \n",
    "        affs_batched = models.conv_pass(unet, ...)\n",
    "        \"\"\"\n",
    "\n",
    "        output_shape_batched = affs_batched.get_shape().as_list()\n",
    "        output_shape = output_shape_batched[1:] # strip the batch dimension\n",
    "\n",
    "        affs = tf.reshape(affs_batched, output_shape)\n",
    "\n",
    "        gt_affs = tf.placeholder(tf.float32, shape=output_shape)\n",
    "        affs_loss_weights = tf.placeholder(tf.float32, shape=output_shape)\n",
    "        \n",
    "        \"\"\"\n",
    "        Define your loss here. For reference see tensorflow.losses documentation.\n",
    "        \n",
    "        loss = ...\n",
    "        \"\"\"\n",
    "        \n",
    "        loss = tf.losses.mean_squared_error(\n",
    "            gt_affs,\n",
    "            affs,\n",
    "            affs_loss_weights)\n",
    "\n",
    "        # Generate a summary for tensorboard:\n",
    "        summary = tf.summary.scalar('setup0', loss)\n",
    "\n",
    "        \"\"\"\n",
    "        Choose an optimizer and learning parameters. Minimize the loss.\n",
    "        For reference see tensorflow.train\n",
    "        \n",
    "        opt = ...\n",
    "        optimizer = opt.minimize(loss)\n",
    "        \"\"\"\n",
    "\n",
    "        output_shape = output_shape[1:]\n",
    "        print(\"input shape : %s\"%(input_shape,))\n",
    "        print(\"output shape: %s\"%(output_shape,))\n",
    "\n",
    "        # Export your computation graph:\n",
    "        tf.train.export_meta_graph(filename=name + '.meta')\n",
    "\n",
    "        # Write out the names of relevant tensors in this graph s.t. the train script can feed and receive values.\n",
    "        config = {\n",
    "            'raw': raw.name,\n",
    "            'affs': affs.name,\n",
    "            'gt_affs': gt_affs.name,\n",
    "            'affs_loss_weights': affs_loss_weights.name,\n",
    "            'loss': loss.name,\n",
    "            'optimizer': optimizer.name,\n",
    "            'input_shape': input_shape,\n",
    "            'output_shape': output_shape,\n",
    "            'summary': summary.name\n",
    "        }\n",
    "\n",
    "        config['outputs'] = {'affs': {\"out_dims\": 3, \"out_dtype\": \"uint8\"}}\n",
    "\n",
    "        with open(name + '.json', 'w') as f:\n",
    "            json.dump(config, f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the script and produce the necessary files for training:\n",
    "    create_network((84, 268, 268), 'train_net')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A gunpowder training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gunpowder.tensorflow.local_server:Creating local tensorflow server\n",
      "INFO:gunpowder.tensorflow.local_server:Server running at b'grpc://localhost:36769'\n",
      "INFO:gunpowder.tensorflow.nodes.train:Initializing tf session, connecting to b'grpc://localhost:36769'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTEXT:  (960, 112, 112)\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gunpowder.tensorflow.nodes.train:Reading meta-graph...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /groups/funke/home/ecksteinn/miniconda2/envs/segmentation/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /groups/funke/home/ecksteinn/miniconda2/envs/segmentation/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "INFO:gunpowder.tensorflow.nodes.train:No checkpoint found\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "from gunpowder import *\n",
    "from gunpowder.tensorflow import *\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "# Adapt the path to where the training data is stored:\n",
    "data_dir = '../data'\n",
    "\n",
    "samples = [\n",
    "    'sample_A',\n",
    "    'sample_B',\n",
    "    'sample_C'\n",
    "]\n",
    "\n",
    "# This defines how we calculate affinities, in this case we consider direct neighbours in z,y,x direction:\n",
    "neighborhood = [[-1, 0, 0], [0, -1, 0], [0, 0, -1]]\n",
    "\n",
    "def train_until(max_iteration):\n",
    "\n",
    "    if tf.train.latest_checkpoint('.'):\n",
    "        trained_until = int(tf.train.latest_checkpoint('.').split('_')[-1])\n",
    "    else:\n",
    "        trained_until = 0\n",
    "    if trained_until >= max_iteration:\n",
    "        return\n",
    "\n",
    "    # Load tensor names from the file you generated above:\n",
    "    with open('train_net.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    # Define gunpowder variables of interest\n",
    "    raw = ArrayKey('RAW')\n",
    "    labels = ArrayKey('GT_LABELS')\n",
    "    labels_mask = ArrayKey('GT_LABELS_MASK')\n",
    "    affs = ArrayKey('PREDICTED_AFFS')\n",
    "    gt = ArrayKey('GT_AFFINITIES')\n",
    "    gt_mask = ArrayKey('GT_AFFINITIES_MASK')\n",
    "    gt_scale = ArrayKey('GT_AFFINITIES_SCALE')\n",
    "    affs_gradient = ArrayKey('AFFS_GRADIENT')\n",
    "\n",
    "    voxel_size = Coordinate((40, 4, 4))\n",
    "    input_size = Coordinate(config['input_shape'])*voxel_size\n",
    "    output_size = Coordinate(config['output_shape'])*voxel_size\n",
    "    context = output_size/2\n",
    "    print('CONTEXT: ', context)\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    Build the actual data loading and train pipeline by chaining\n",
    "    gunpowder nodes. Each gunpowder node provides an atomic operation \n",
    "    and can request data from upstream nodes. For reference see:\n",
    "    https://github.com/funkey/gunpowder\n",
    "    \n",
    "    In the following we provide you with a skeleton\n",
    "    pipeline. You should add the actual training node, data preprocessing nodes and \n",
    "    data augmentation nodes.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Request all the data you need for training:\n",
    "    request = BatchRequest()\n",
    "    request.add(raw, input_size)\n",
    "    request.add(labels, output_size)\n",
    "    request.add(labels_mask, output_size)\n",
    "    request.add(gt, output_size)\n",
    "    request.add(gt_mask, output_size)\n",
    "    request.add(gt_scale, output_size)\n",
    "\n",
    "    # Request a snapshot s.t. you are able to visualize the data used and produced during training.\n",
    "    snapshot_request = BatchRequest({\n",
    "        affs: request[gt],\n",
    "        affs_gradient: request[gt]\n",
    "    })\n",
    "\n",
    "    # Define a data source. In this case the data we provide contains \n",
    "    # raw EM images together with neuron segments (neuron_ids)\n",
    "    # and a mask for regions in the EM data that should not contribute towards \n",
    "    # training.\n",
    "    data_sources = tuple(\n",
    "        N5Source(\n",
    "            os.path.join(data_dir, sample + '.n5'),\n",
    "            datasets = {\n",
    "                raw: 'volumes/raw',\n",
    "                labels: 'volumes/labels/neuron_ids',\n",
    "                labels_mask: 'volumes/labels/mask',\n",
    "            },\n",
    "            array_specs = {\n",
    "                raw: ArraySpec(interpolatable=True),\n",
    "                labels: ArraySpec(interpolatable=False),\n",
    "                labels_mask: ArraySpec(interpolatable=False)\n",
    "            }\n",
    "        ) +\n",
    "        Normalize(raw) + \n",
    "        Pad(labels, context) +\n",
    "        Pad(labels_mask, context) +\n",
    "        RandomLocation() +\n",
    "        Reject(mask=labels_mask)\n",
    "        for sample in samples\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    Define the actual train pipeline here:\n",
    "    \n",
    "    train_pipeline = (\n",
    "        data_sources +\n",
    "        RandomProvider() +\n",
    "        ```Add elastic augmentation``` +\n",
    "        ```Add simple augmentation``` + \n",
    "        ```Add intensity augmentation``` +\n",
    "        GrowBoundary(labels, labels_mask, steps=1, only_xy=True) +\n",
    "        ```So far the data sources only provide neuron labels\n",
    "            and the raw em data. We need to generate the actual training\n",
    "            data which are affinities. Add a node that does this here.``` +\n",
    "        BalanceLabels(\n",
    "            gt,\n",
    "            gt_scale,\n",
    "            gt_mask) +\n",
    "        ```Add defect augment without artifacts```\n",
    "        IntensityScaleShift(raw, 2,-1) +\n",
    "        PreCache(cache_size=40,\n",
    "                 num_workers=10) +\n",
    "        ```\n",
    "        Add the actual Train node here:\n",
    "        Train(...) +\n",
    "        ```\n",
    "        IntensityScaleShift(raw, 0.5, 0.5) +\n",
    "        Snapshot({\n",
    "                raw: 'volumes/raw',\n",
    "                labels: 'volumes/labels/neuron_ids',\n",
    "                gt: 'volumes/gt_affinities',\n",
    "                affs: 'volumes/pred_affinities',\n",
    "                gt_mask: 'volumes/labels/gt_mask',\n",
    "                labels_mask: 'volumes/labels/mask',\n",
    "                affs_gradient: 'volumes/affs_gradient'\n",
    "            },\n",
    "            dataset_dtypes={\n",
    "                labels: np.uint64\n",
    "            },\n",
    "            every=1000,\n",
    "            output_filename='batch_{iteration}.hdf',\n",
    "            additional_request=snapshot_request) +\n",
    "        PrintProfilingStats(every=10)\n",
    "    )\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    \"\"\"\n",
    "    Implement the train loop:\n",
    "    with build(train_pipeline) as b:\n",
    "        ...\n",
    "    \"\"\"\n",
    "    print(\"Training finished\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iteration = 500000\n",
    "    train_until(iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
